{
  "servicePath": "",
  "icons": {
    "x32": "http://www.google.com/images/icons/product/search-32.gif",
    "x16": "http://www.google.com/images/icons/product/search-16.gif"
  },
  "ownerDomain": "google.com",
  "resources": {
    "documents": {
      "methods": {
        "analyzeEntities": {
          "parameters": {},
          "parameterOrder": [],
          "response": {
            "$ref": "AnalyzeEntitiesResponse"
          },
          "description": "Finds named entities (currently proper names and common nouns) in the text along with entity types, salience, mentions for each entity, and other properties.",
          "id": "language.documents.analyzeEntities",
          "path": "v1beta2/documents:analyzeEntities",
          "flatPath": "v1beta2/documents:analyzeEntities",
          "scopes": [
            "https://www.googleapis.com/auth/cloud-language",
            "https://www.googleapis.com/auth/cloud-platform"
          ],
          "httpMethod": "POST",
          "request": {
            "$ref": "AnalyzeEntitiesRequest"
          }
        },
        "analyzeEntitySentiment": {
          "response": {
            "$ref": "AnalyzeEntitySentimentResponse"
          },
          "path": "v1beta2/documents:analyzeEntitySentiment",
          "scopes": [
            "https://www.googleapis.com/auth/cloud-language",
            "https://www.googleapis.com/auth/cloud-platform"
          ],
          "httpMethod": "POST",
          "description": "Finds entities, similar to AnalyzeEntities in the text and analyzes sentiment associated with each entity and its mentions.",
          "id": "language.documents.analyzeEntitySentiment",
          "parameters": {},
          "flatPath": "v1beta2/documents:analyzeEntitySentiment",
          "parameterOrder": [],
          "request": {
            "$ref": "AnalyzeEntitySentimentRequest"
          }
        },
        "annotateText": {
          "description": "A convenience method that provides all syntax, sentiment, entity, and classification features in one call.",
          "scopes": [
            "https://www.googleapis.com/auth/cloud-language",
            "https://www.googleapis.com/auth/cloud-platform"
          ],
          "request": {
            "$ref": "AnnotateTextRequest"
          },
          "response": {
            "$ref": "AnnotateTextResponse"
          },
          "parameters": {},
          "flatPath": "v1beta2/documents:annotateText",
          "id": "language.documents.annotateText",
          "httpMethod": "POST",
          "parameterOrder": [],
          "path": "v1beta2/documents:annotateText"
        },
        "analyzeSyntax": {
          "path": "v1beta2/documents:analyzeSyntax",
          "id": "language.documents.analyzeSyntax",
          "parameters": {},
          "request": {
            "$ref": "AnalyzeSyntaxRequest"
          },
          "parameterOrder": [],
          "httpMethod": "POST",
          "description": "Analyzes the syntax of the text and provides sentence boundaries and tokenization along with part of speech tags, dependency trees, and other properties.",
          "scopes": [
            "https://www.googleapis.com/auth/cloud-language",
            "https://www.googleapis.com/auth/cloud-platform"
          ],
          "flatPath": "v1beta2/documents:analyzeSyntax",
          "response": {
            "$ref": "AnalyzeSyntaxResponse"
          }
        },
        "classifyText": {
          "scopes": [
            "https://www.googleapis.com/auth/cloud-language",
            "https://www.googleapis.com/auth/cloud-platform"
          ],
          "flatPath": "v1beta2/documents:classifyText",
          "parameterOrder": [],
          "id": "language.documents.classifyText",
          "description": "Classifies a document into categories.",
          "httpMethod": "POST",
          "path": "v1beta2/documents:classifyText",
          "parameters": {},
          "request": {
            "$ref": "ClassifyTextRequest"
          },
          "response": {
            "$ref": "ClassifyTextResponse"
          }
        },
        "moderateText": {
          "flatPath": "v1beta2/documents:moderateText",
          "parameterOrder": [],
          "parameters": {},
          "path": "v1beta2/documents:moderateText",
          "request": {
            "$ref": "ModerateTextRequest"
          },
          "scopes": [
            "https://www.googleapis.com/auth/cloud-language",
            "https://www.googleapis.com/auth/cloud-platform"
          ],
          "httpMethod": "POST",
          "response": {
            "$ref": "ModerateTextResponse"
          },
          "id": "language.documents.moderateText",
          "description": "Moderates a document for harmful and sensitive categories."
        },
        "analyzeSentiment": {
          "scopes": [
            "https://www.googleapis.com/auth/cloud-language",
            "https://www.googleapis.com/auth/cloud-platform"
          ],
          "id": "language.documents.analyzeSentiment",
          "parameterOrder": [],
          "httpMethod": "POST",
          "response": {
            "$ref": "AnalyzeSentimentResponse"
          },
          "parameters": {},
          "path": "v1beta2/documents:analyzeSentiment",
          "description": "Analyzes the sentiment of the provided text.",
          "request": {
            "$ref": "AnalyzeSentimentRequest"
          },
          "flatPath": "v1beta2/documents:analyzeSentiment"
        }
      }
    }
  },
  "ownerName": "Google",
  "documentationLink": "https://cloud.google.com/natural-language/",
  "rootUrl": "https://language.googleapis.com/",
  "fullyEncodeReservedExpansion": true,
  "kind": "discovery#restDescription",
  "schemas": {
    "XPSRegressionEvaluationMetrics": {
      "description": "Model evaluation metrics for regression problems. It can be used for Tables.",
      "type": "object",
      "properties": {
        "meanAbsoluteError": {
          "type": "number",
          "format": "float",
          "description": "Mean Absolute Error (MAE)."
        },
        "regressionMetricsEntries": {
          "type": "array",
          "items": {
            "$ref": "XPSRegressionMetricsEntry"
          },
          "description": "A list of actual versus predicted points for the model being evaluated."
        },
        "meanAbsolutePercentageError": {
          "format": "float",
          "type": "number",
          "description": "Mean absolute percentage error. Only set if all ground truth values are positive."
        },
        "rootMeanSquaredError": {
          "format": "float",
          "type": "number",
          "description": "Root Mean Squared Error (RMSE)."
        },
        "rSquared": {
          "description": "R squared.",
          "type": "number",
          "format": "float"
        },
        "rootMeanSquaredLogError": {
          "type": "number",
          "format": "float",
          "description": "Root mean squared log error."
        }
      },
      "id": "XPSRegressionEvaluationMetrics"
    },
    "ClassifyTextRequest": {
      "properties": {
        "document": {
          "$ref": "Document",
          "description": "Required. Input document."
        },
        "classificationModelOptions": {
          "$ref": "ClassificationModelOptions",
          "description": "Optional. Model options to use for classification. Defaults to v1 options if not specified."
        }
      },
      "id": "ClassifyTextRequest",
      "type": "object",
      "description": "The document classification request message."
    },
    "Token": {
      "properties": {
        "lemma": {
          "type": "string",
          "description": "[Lemma](https://en.wikipedia.org/wiki/Lemma_%28morphology%29) of the token."
        },
        "dependencyEdge": {
          "description": "Dependency tree parse for this token.",
          "$ref": "DependencyEdge"
        },
        "partOfSpeech": {
          "description": "Parts of speech tag for this token.",
          "$ref": "PartOfSpeech"
        },
        "text": {
          "$ref": "TextSpan",
          "description": "The token text."
        }
      },
      "type": "object",
      "id": "Token",
      "description": "Represents the smallest syntactic building block of the text."
    },
    "XPSTextTrainResponse": {
      "properties": {
        "componentModel": {
          "description": "Component submodels.",
          "items": {
            "$ref": "XPSTextComponentModel"
          },
          "type": "array"
        }
      },
      "type": "object",
      "id": "XPSTextTrainResponse"
    },
    "XPSSpeechModelSpec": {
      "id": "XPSSpeechModelSpec",
      "properties": {
        "subModelSpecs": {
          "items": {
            "$ref": "XPSSpeechModelSpecSubModelSpec"
          },
          "description": "Model specs for all submodels contained in this model.",
          "type": "array"
        },
        "language": {
          "type": "string"
        },
        "datasetId": {
          "format": "int64",
          "type": "string",
          "description": "Required for speech xps backend. Speech xps has to use dataset_id and model_id as the primary key in db so that speech API can query the db directly."
        }
      },
      "type": "object"
    },
    "XPSCorrelationStats": {
      "description": "A correlation statistics between two series of DataType values. The series may have differing DataType-s, but within a single series the DataType must be the same.",
      "id": "XPSCorrelationStats",
      "type": "object",
      "properties": {
        "cramersV": {
          "format": "double",
          "description": "The correlation value using the Cramer's V measure.",
          "type": "number"
        }
      }
    },
    "Sentence": {
      "properties": {
        "sentiment": {
          "$ref": "Sentiment",
          "description": "For calls to AnalyzeSentiment or if AnnotateTextRequest.Features.extract_document_sentiment is set to true, this field will contain the sentiment for the sentence."
        },
        "text": {
          "description": "The sentence text.",
          "$ref": "TextSpan"
        }
      },
      "id": "Sentence",
      "type": "object",
      "description": "Represents a sentence in the input document."
    },
    "XPSMetricEntry": {
      "type": "object",
      "id": "XPSMetricEntry",
      "properties": {
        "doubleValue": {
          "type": "number",
          "format": "double",
          "description": "A double value."
        },
        "systemLabels": {
          "type": "array",
          "items": {
            "$ref": "XPSMetricEntryLabel"
          },
          "description": "Billing system labels for this (metric, value) pair."
        },
        "int64Value": {
          "description": "A signed 64-bit integer value.",
          "format": "int64",
          "type": "string"
        },
        "argentumMetricId": {
          "description": "For billing metrics that are using legacy sku's, set the legacy billing metric id here. This will be sent to Chemist as the \"cloudbilling.googleapis.com/argentum_metric_id\" label. Otherwise leave empty.",
          "type": "string"
        },
        "metricName": {
          "description": "The metric name defined in the service configuration.",
          "type": "string"
        }
      }
    },
    "XPSEvaluationMetricsSet": {
      "id": "XPSEvaluationMetricsSet",
      "description": "Specifies location of model evaluation metrics.",
      "properties": {
        "evaluationMetrics": {
          "type": "array",
          "items": {
            "$ref": "XPSEvaluationMetrics"
          },
          "description": "Inline EvaluationMetrics - should be relatively small. For passing large quantities of exhaustive metrics, use file_spec."
        },
        "numEvaluationMetrics": {
          "description": "Number of the evaluation metrics (usually one per label plus overall).",
          "format": "int64",
          "type": "string"
        },
        "fileSpec": {
          "description": "File spec containing evaluation metrics of a model, must point to RecordIO file(s) of intelligence.cloud.automl.xps.EvaluationMetrics messages.",
          "$ref": "XPSFileSpec"
        }
      },
      "type": "object"
    },
    "XPSStructStats": {
      "properties": {
        "fieldStats": {
          "description": "Map from a field name of the struct to data stats aggregated over series of all data in that field across all the structs.",
          "type": "object",
          "additionalProperties": {
            "$ref": "XPSDataStats"
          }
        },
        "commonStats": {
          "$ref": "XPSCommonStats"
        }
      },
      "type": "object",
      "id": "XPSStructStats",
      "description": "The data statistics of a series of STRUCT values."
    },
    "XPSImageSegmentationEvaluationMetricsConfidenceMetricsEntry": {
      "description": "Metrics for a single confidence threshold.",
      "id": "XPSImageSegmentationEvaluationMetricsConfidenceMetricsEntry",
      "type": "object",
      "properties": {
        "diceScoreCoefficient": {
          "format": "float",
          "type": "number",
          "description": "DSC or the F1 score: The harmonic mean of recall and precision."
        },
        "recall": {
          "description": "Recall for the given confidence threshold.",
          "type": "number",
          "format": "float"
        },
        "precision": {
          "description": "Precision for the given confidence threshold.",
          "format": "float",
          "type": "number"
        },
        "iouScore": {
          "format": "float",
          "type": "number",
          "description": "IOU score."
        },
        "confidenceThreshold": {
          "type": "number",
          "format": "float",
          "description": "The confidence threshold value used to compute the metrics."
        },
        "confusionMatrix": {
          "description": "Confusion matrix of the per confidence_threshold evaluation. Pixel counts are set here. Only set for model level evaluation, not for evaluation per label.",
          "$ref": "XPSConfusionMatrix"
        }
      }
    },
    "XPSVideoClassificationTrainResponse": {
      "id": "XPSVideoClassificationTrainResponse",
      "type": "object",
      "properties": {
        "trainCostNodeSeconds": {
          "type": "string",
          "format": "int64",
          "description": "The actual train cost of creating this model, expressed in node seconds, i.e. 3,600 value in this field means 1 node hour."
        },
        "modelArtifactSpec": {
          "description": "## The fields below are only populated under uCAIP request scope.",
          "$ref": "XPSVideoModelArtifactSpec"
        }
      }
    },
    "XPSFloat64StatsHistogramBucket": {
      "id": "XPSFloat64StatsHistogramBucket",
      "description": "A bucket of a histogram.",
      "properties": {
        "count": {
          "type": "string",
          "description": "The number of data values that are in the bucket, i.e. are between min and max values.",
          "format": "int64"
        },
        "max": {
          "type": "number",
          "description": "The maximum value of the bucket, exclusive unless max = `\"Infinity\"`, in which case it's inclusive.",
          "format": "double"
        },
        "min": {
          "type": "number",
          "description": "The minimum value of the bucket, inclusive.",
          "format": "double"
        }
      },
      "type": "object"
    },
    "InfraUsage": {
      "type": "object",
      "properties": {
        "tpuMetrics": {
          "description": "Aggregated tpu metrics since requested start_time.",
          "type": "array",
          "items": {
            "$ref": "TpuMetric"
          }
        },
        "diskMetrics": {
          "type": "array",
          "items": {
            "$ref": "DiskMetric"
          },
          "description": "Aggregated persistent disk metrics since requested start_time."
        },
        "cpuMetrics": {
          "description": "Aggregated core metrics since requested start_time.",
          "type": "array",
          "items": {
            "$ref": "CpuMetric"
          }
        },
        "gpuMetrics": {
          "type": "array",
          "items": {
            "$ref": "GpuMetric"
          },
          "description": "Aggregated gpu metrics since requested start_time."
        },
        "ramMetrics": {
          "description": "Aggregated ram metrics since requested start_time.",
          "items": {
            "$ref": "RamMetric"
          },
          "type": "array"
        }
      },
      "description": "Infra Usage of billing metrics. Next ID: 6",
      "id": "InfraUsage"
    },
    "XPSCategoryStats": {
      "properties": {
        "commonStats": {
          "$ref": "XPSCommonStats"
        },
        "topCategoryStats": {
          "type": "array",
          "description": "The statistics of the top 20 CATEGORY values, ordered by CategoryStats.SingleCategoryStats.count.",
          "items": {
            "$ref": "XPSCategoryStatsSingleCategoryStats"
          }
        }
      },
      "id": "XPSCategoryStats",
      "type": "object",
      "description": "The data statistics of a series of CATEGORY values."
    },
    "XPSSpeechPreprocessStats": {
      "type": "object",
      "properties": {
        "dataErrors": {
          "description": "Different types of data errors and the counts associated with them.",
          "items": {
            "$ref": "XPSDataErrors"
          },
          "type": "array"
        },
        "trainExamplesCount": {
          "type": "integer",
          "format": "int32",
          "description": "The number of examples labeled as TRAIN by Speech xps server."
        },
        "numLogsExamples": {
          "format": "int32",
          "type": "integer",
          "description": "The number of samples found in the previously recorded logs data."
        },
        "testExamplesCount": {
          "type": "integer",
          "format": "int32",
          "description": "The number of examples labelled as TEST by Speech xps server."
        },
        "testWordsCount": {
          "format": "int32",
          "description": "The number of words in the test data set.",
          "type": "integer"
        },
        "numHumanLabeledExamples": {
          "type": "integer",
          "format": "int32",
          "description": "The number of rows marked HUMAN_LABELLED"
        },
        "trainSentencesCount": {
          "format": "int32",
          "description": "The number of sentences in the training data set.",
          "type": "integer"
        },
        "trainWordsCount": {
          "format": "int32",
          "description": "The number of words in the training data set.",
          "type": "integer"
        },
        "testSentencesCount": {
          "format": "int32",
          "type": "integer",
          "description": "The number of sentences in the test data set."
        },
        "numMachineTranscribedExamples": {
          "format": "int32",
          "description": "The number of rows marked as MACHINE_TRANSCRIBED",
          "type": "integer"
        }
      },
      "id": "XPSSpeechPreprocessStats"
    },
    "XPSArrayStats": {
      "description": "The data statistics of a series of ARRAY values.",
      "type": "object",
      "properties": {
        "commonStats": {
          "$ref": "XPSCommonStats"
        },
        "memberStats": {
          "description": "Stats of all the values of all arrays, as if they were a single long series of data. The type depends on the element type of the array.",
          "$ref": "XPSDataStats"
        }
      },
      "id": "XPSArrayStats"
    },
    "XPSClassificationEvaluationMetrics": {
      "type": "object",
      "properties": {
        "baseAuPrc": {
          "description": "The Area under precision recall curve metric based on priors.",
          "format": "float",
          "type": "number"
        },
        "logLoss": {
          "description": "The Log Loss metric.",
          "format": "float",
          "type": "number"
        },
        "confidenceMetricsEntries": {
          "type": "array",
          "items": {
            "$ref": "XPSConfidenceMetricsEntry"
          },
          "description": "Metrics that have confidence thresholds. Precision-recall curve can be derived from it."
        },
        "auPrc": {
          "description": "The Area under precision recall curve metric.",
          "format": "float",
          "type": "number"
        },
        "confusionMatrix": {
          "$ref": "XPSConfusionMatrix",
          "description": "Confusion matrix of the evaluation. Only set for MULTICLASS classification problems where number of annotation specs is no more than 10. Only set for model level evaluation, not for evaluation per label."
        },
        "evaluatedExamplesCount": {
          "format": "int32",
          "description": "The number of examples used for model evaluation.",
          "type": "integer"
        },
        "auRoc": {
          "type": "number",
          "description": "The Area Under Receiver Operating Characteristic curve metric. Micro-averaged for the overall evaluation.",
          "format": "float"
        }
      },
      "id": "XPSClassificationEvaluationMetrics",
      "description": "Model evaluation metrics for classification problems. It can be used for image and video classification. Next tag: 9."
    },
    "TpuMetric": {
      "properties": {
        "tpuSec": {
          "type": "string",
          "description": "Required. Seconds of TPU usage, e.g. 3600.",
          "format": "int64"
        },
        "tpuType": {
          "type": "string",
          "enumDescriptions": [
            "",
            "",
            "",
            "",
            "",
            ""
          ],
          "description": "Required. Type of TPU, e.g. TPU_V2, TPU_V3_POD.",
          "enum": [
            "UNKNOWN_TPU_TYPE",
            "TPU_V2_POD",
            "TPU_V2",
            "TPU_V3_POD",
            "TPU_V3",
            "TPU_V5_LITEPOD"
          ]
        }
      },
      "type": "object",
      "id": "TpuMetric"
    },
    "XPSDockerFormat": {
      "description": "A model format used for Docker containers. Use the params field to customize the container. The container is verified to work correctly on ubuntu 16.04 operating system.",
      "properties": {
        "gpuArchitecture": {
          "enum": [
            "GPU_ARCHITECTURE_UNSPECIFIED",
            "GPU_ARCHITECTURE_NVIDIA"
          ],
          "description": "Optional. Additional gpu information describing the requirements for the to be exported model files.",
          "enumDescriptions": [
            "",
            ""
          ],
          "type": "string"
        },
        "cpuArchitecture": {
          "type": "string",
          "enumDescriptions": [
            "",
            ""
          ],
          "description": "Optional. Additional cpu information describing the requirements for the to be exported model files.",
          "enum": [
            "CPU_ARCHITECTURE_UNSPECIFIED",
            "CPU_ARCHITECTURE_X86_64"
          ]
        }
      },
      "id": "XPSDockerFormat",
      "type": "object"
    },
    "XPSTrainingObjectivePoint": {
      "id": "XPSTrainingObjectivePoint",
      "type": "object",
      "properties": {
        "createTime": {
          "description": "The time at which this point was recorded.",
          "type": "string",
          "format": "google-datetime"
        },
        "value": {
          "format": "float",
          "type": "number",
          "description": "The objective value when this point was recorded."
        }
      }
    },
    "XPSVideoActionMetricsEntryConfidenceMetricsEntry": {
      "description": "Metrics for a single confidence threshold.",
      "type": "object",
      "id": "XPSVideoActionMetricsEntryConfidenceMetricsEntry",
      "properties": {
        "confidenceThreshold": {
          "type": "number",
          "description": "Output only. The confidence threshold value used to compute the metrics.",
          "format": "float"
        },
        "precision": {
          "format": "float",
          "description": "Output only. Precision for the given confidence threshold.",
          "type": "number"
        },
        "recall": {
          "format": "float",
          "description": "Output only. Recall for the given confidence threshold.",
          "type": "number"
        },
        "f1Score": {
          "format": "float",
          "description": "Output only. The harmonic mean of recall and precision.",
          "type": "number"
        }
      }
    },
    "AnalyzeEntitiesRequest": {
      "id": "AnalyzeEntitiesRequest",
      "description": "The entity analysis request message.",
      "properties": {
        "document": {
          "$ref": "Document",
          "description": "Required. Input document."
        },
        "encodingType": {
          "description": "The encoding type used by the API to calculate offsets.",
          "enumDescriptions": [
            "If `EncodingType` is not specified, encoding-dependent information (such as `begin_offset`) will be set at `-1`.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-8 encoding of the input. C++ and Go are examples of languages that use this encoding natively.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-16 encoding of the input. Java and JavaScript are examples of languages that use this encoding natively.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-32 encoding of the input. Python is an example of a language that uses this encoding natively."
          ],
          "type": "string",
          "enum": [
            "NONE",
            "UTF8",
            "UTF16",
            "UTF32"
          ]
        }
      },
      "type": "object"
    },
    "XPSTablesTrainingOperationMetadata": {
      "properties": {
        "trainingStartTime": {
          "format": "google-datetime",
          "type": "string",
          "description": "Timestamp when training process starts."
        },
        "optimizationObjective": {
          "type": "string",
          "description": "The optimization objective for model."
        },
        "topTrials": {
          "description": "This field is for training. When the operation is terminated successfully, AutoML Backend post this field to operation metadata in spanner. If the metadata has no trials returned, the training operation is supposed to be a failure.",
          "type": "array",
          "items": {
            "$ref": "XPSTuningTrial"
          }
        },
        "trainBudgetMilliNodeHours": {
          "description": "Creating model budget.",
          "type": "string",
          "format": "int64"
        },
        "createModelStage": {
          "type": "string",
          "description": "Current stage of creating model.",
          "enum": [
            "CREATE_MODEL_STAGE_UNSPECIFIED",
            "DATA_PREPROCESSING",
            "TRAINING",
            "EVALUATING",
            "MODEL_POST_PROCESSING"
          ],
          "enumDescriptions": [
            "Unspecified stage.",
            "Prepare the model training pipeline and run data processing.",
            "Training model.",
            "Run evaluation.",
            "Finalizing model training pipeline."
          ]
        },
        "trainingObjectivePoints": {
          "type": "array",
          "description": "This field records the training objective value with respect to time, giving insight into how the model architecture search is performing as training time elapses.",
          "items": {
            "$ref": "XPSTrainingObjectivePoint"
          }
        }
      },
      "id": "XPSTablesTrainingOperationMetadata",
      "type": "object"
    },
    "XPSCommonStats": {
      "id": "XPSCommonStats",
      "properties": {
        "distinctValueCount": {
          "type": "string",
          "format": "int64"
        },
        "validValueCount": {
          "format": "int64",
          "type": "string"
        },
        "nullValueCount": {
          "format": "int64",
          "type": "string"
        }
      },
      "description": "Common statistics for a column with a specified data type.",
      "type": "object"
    },
    "ClassificationModelOptionsV1Model": {
      "type": "object",
      "properties": {},
      "id": "ClassificationModelOptionsV1Model",
      "description": "Options for the V1 model."
    },
    "XPSDataErrors": {
      "id": "XPSDataErrors",
      "type": "object",
      "description": "Different types of errors and the stats associatesd with each error.",
      "properties": {
        "errorType": {
          "type": "string",
          "enum": [
            "ERROR_TYPE_UNSPECIFIED",
            "UNSUPPORTED_AUDIO_FORMAT",
            "FILE_EXTENSION_MISMATCH_WITH_AUDIO_FORMAT",
            "FILE_TOO_LARGE",
            "MISSING_TRANSCRIPTION"
          ],
          "description": "Type of the error.",
          "enumDescriptions": [
            "Not specified.",
            "Audio format not in the formats by cloud-speech AutoML. Currently only wav and flac file formats are supported.",
            "File format differnt from what is specified in the file name extension.",
            "File too large. Maximum allowed size is 50 MB.",
            "Transcript is missing."
          ]
        },
        "count": {
          "description": "Number of records having errors associated with the enum.",
          "type": "integer",
          "format": "int32"
        }
      }
    },
    "XPSStringStatsUnigramStats": {
      "id": "XPSStringStatsUnigramStats",
      "properties": {
        "value": {
          "type": "string",
          "description": "The unigram."
        },
        "count": {
          "type": "string",
          "description": "The number of occurrences of this unigram in the series.",
          "format": "int64"
        }
      },
      "description": "The statistics of a unigram.",
      "type": "object"
    },
    "XPSColorMap": {
      "id": "XPSColorMap",
      "type": "object",
      "properties": {
        "color": {
          "description": "This type is deprecated in favor of the IntColor below. This is because google.type.Color represent color has a float which semantically does not reflect discrete classes/categories concept. Moreover, to handle it well we need to have some tolerance when converting to a discretized color. As such, the recommendation is to have API surface still use google.type.Color while internally IntColor is used.",
          "$ref": "Color",
          "deprecated": true
        },
        "displayName": {
          "description": "Should be used during preprocessing.",
          "type": "string"
        },
        "intColor": {
          "$ref": "XPSColorMapIntColor"
        },
        "annotationSpecIdToken": {
          "type": "string",
          "description": "Should be used during training."
        }
      },
      "description": "Map from color to display name. Will only be used by Image Segmentation for uCAIP."
    },
    "XPSTablesModelColumnInfo": {
      "id": "XPSTablesModelColumnInfo",
      "type": "object",
      "properties": {
        "featureImportance": {
          "format": "float",
          "type": "number",
          "description": "When given as part of a Model: Measurement of how much model predictions correctness on the TEST data depend on values in this column. A value between 0 and 1, higher means higher influence. These values are normalized - for all input feature columns of a given model they add to 1. When given back by Predict or Batch Predict: Measurement of how impactful for the prediction returned for the given row the value in this column was. Specifically, the feature importance specifies the marginal contribution that the feature made to the prediction score compared to the baseline score. These values are computed using the Sampled Shapley method."
        },
        "columnId": {
          "type": "integer",
          "format": "int32",
          "description": "The ID of the column."
        }
      },
      "description": "An information specific to given column and Tables Model, in context of the Model and the predictions created by it."
    },
    "Document": {
      "properties": {
        "language": {
          "type": "string",
          "description": "The language of the document (if not specified, the language is automatically detected). Both ISO and BCP-47 language codes are accepted. [Language Support](https://cloud.google.com/natural-language/docs/languages) lists currently supported languages for each API method. If the language (either specified by the caller or automatically detected) is not supported by the called API method, an `INVALID_ARGUMENT` error is returned."
        },
        "gcsContentUri": {
          "description": "The Google Cloud Storage URI where the file content is located. This URI must be of the form: gs://bucket_name/object_name. For more details, see https://cloud.google.com/storage/docs/reference-uris. NOTE: Cloud Storage object versioning is not supported.",
          "type": "string"
        },
        "referenceWebUri": {
          "description": "The web URI where the document comes from. This URI is not used for fetching the content, but as a hint for analyzing the document.",
          "type": "string"
        },
        "content": {
          "description": "The content of the input in string format. Cloud audit logging exempt since it is based on user data.",
          "type": "string"
        },
        "boilerplateHandling": {
          "enumDescriptions": [
            "The boilerplate handling is not specified.",
            "Do not analyze detected boilerplate. Reference web URI is required for detecting boilerplate.",
            "Treat boilerplate the same as content."
          ],
          "type": "string",
          "description": "Indicates how detected boilerplate(e.g. advertisements, copyright declarations, banners) should be handled for this document. If not specified, boilerplate will be treated the same as content.",
          "enum": [
            "BOILERPLATE_HANDLING_UNSPECIFIED",
            "SKIP_BOILERPLATE",
            "KEEP_BOILERPLATE"
          ]
        },
        "type": {
          "type": "string",
          "enumDescriptions": [
            "The content type is not specified.",
            "Plain text",
            "HTML"
          ],
          "enum": [
            "TYPE_UNSPECIFIED",
            "PLAIN_TEXT",
            "HTML"
          ],
          "description": "Required. If the type is not set or is `TYPE_UNSPECIFIED`, returns an `INVALID_ARGUMENT` error."
        }
      },
      "description": "Represents the input to API methods.",
      "type": "object",
      "id": "Document"
    },
    "XPSVideoTrainingOperationMetadata": {
      "properties": {
        "trainCostMilliNodeHour": {
          "format": "int64",
          "type": "string",
          "description": "This is an estimation of the node hours necessary for training a model, expressed in milli node hours (i.e. 1,000 value in this field means 1 node hour). A node hour represents the time a virtual machine spends running your training job. The cost of one node running for one hour is a node hour."
        }
      },
      "id": "XPSVideoTrainingOperationMetadata",
      "type": "object"
    },
    "XPSImageModelServingSpec": {
      "description": "Serving specification for image models.",
      "properties": {
        "tfRuntimeVersion": {
          "description": "## The fields below are only populated under uCAIP request scope. https://cloud.google.com/ml-engine/docs/runtime-version-list",
          "type": "string"
        },
        "nodeQps": {
          "type": "number",
          "format": "double",
          "description": "An estimated value of how much traffic a node can serve. Populated for AutoMl request only."
        },
        "modelThroughputEstimation": {
          "description": "Populate under uCAIP request scope.",
          "items": {
            "$ref": "XPSImageModelServingSpecModelThroughputEstimation"
          },
          "type": "array"
        }
      },
      "type": "object",
      "id": "XPSImageModelServingSpec"
    },
    "XPSSpeechEvaluationMetricsSubModelEvaluationMetric": {
      "properties": {
        "numSubstitutions": {
          "format": "int32",
          "type": "integer"
        },
        "numDeletions": {
          "type": "integer",
          "format": "int32"
        },
        "numWords": {
          "format": "int32",
          "description": "Number of words over which the word error rate was computed.",
          "type": "integer"
        },
        "numUtterances": {
          "format": "int32",
          "type": "integer",
          "description": "Number of utterances used in the wer computation."
        },
        "biasingModelType": {
          "type": "string",
          "description": "Type of the biasing model.",
          "enumDescriptions": [
            "",
            "Build biasing model on top of COMMAND_AND_SEARCH model",
            "Build biasing model on top of PHONE_CALL model",
            "Build biasing model on top of VIDEO model",
            "Build biasing model on top of DEFAULT model"
          ],
          "enum": [
            "BIASING_MODEL_TYPE_UNSPECIFIED",
            "COMMAND_AND_SEARCH",
            "PHONE_CALL",
            "VIDEO",
            "DEFAULT"
          ]
        },
        "isEnhancedModel": {
          "description": "If true then it means we have an enhanced version of the biasing models.",
          "type": "boolean"
        },
        "wer": {
          "description": "Word error rate (standard error metric used for speech recognition).",
          "type": "number",
          "format": "double"
        },
        "numInsertions": {
          "format": "int32",
          "type": "integer"
        },
        "sentenceAccuracy": {
          "format": "double",
          "description": "Below fields are used for debugging purposes",
          "type": "number"
        }
      },
      "id": "XPSSpeechEvaluationMetricsSubModelEvaluationMetric",
      "type": "object"
    },
    "XPSImageObjectDetectionModelSpec": {
      "properties": {
        "exportModelSpec": {
          "$ref": "XPSImageExportModelSpec"
        },
        "trainCostNodeSeconds": {
          "type": "string",
          "format": "int64",
          "description": "The actual train cost of creating this model, expressed in node seconds, i.e. 3,600 value in this field means 1 node hour."
        },
        "modelServingSpec": {
          "$ref": "XPSImageModelServingSpec"
        },
        "maxBoundingBoxCount": {
          "description": "Max number of bounding box.",
          "type": "string",
          "format": "int64"
        },
        "classCount": {
          "description": "Total number of classes.",
          "format": "int64",
          "type": "string"
        },
        "modelArtifactSpec": {
          "$ref": "XPSImageModelArtifactSpec",
          "description": "## The fields below are only populated under uCAIP request scope."
        },
        "stopReason": {
          "enum": [
            "TRAIN_STOP_REASON_UNSPECIFIED",
            "TRAIN_STOP_REASON_BUDGET_REACHED",
            "TRAIN_STOP_REASON_MODEL_CONVERGED",
            "TRAIN_STOP_REASON_MODEL_EARLY_STOPPED"
          ],
          "enumDescriptions": [
            "",
            "",
            "Model fully converged, can not be resumbed training.",
            "Model early converged, can be further trained till full convergency."
          ],
          "description": "Stop reason for training job, e.g. 'TRAIN_BUDGET_REACHED', 'MODEL_CONVERGED'.",
          "type": "string"
        }
      },
      "id": "XPSImageObjectDetectionModelSpec",
      "type": "object"
    },
    "AnalyzeSentimentRequest": {
      "id": "AnalyzeSentimentRequest",
      "properties": {
        "document": {
          "$ref": "Document",
          "description": "Required. Input document."
        },
        "encodingType": {
          "enumDescriptions": [
            "If `EncodingType` is not specified, encoding-dependent information (such as `begin_offset`) will be set at `-1`.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-8 encoding of the input. C++ and Go are examples of languages that use this encoding natively.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-16 encoding of the input. Java and JavaScript are examples of languages that use this encoding natively.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-32 encoding of the input. Python is an example of a language that uses this encoding natively."
          ],
          "description": "The encoding type used by the API to calculate sentence offsets for the sentence sentiment.",
          "type": "string",
          "enum": [
            "NONE",
            "UTF8",
            "UTF16",
            "UTF32"
          ]
        }
      },
      "type": "object",
      "description": "The sentiment analysis request message."
    },
    "CpuMetric": {
      "type": "object",
      "id": "CpuMetric",
      "properties": {
        "trackingLabels": {
          "description": "Billing tracking labels. They do not contain any user data but only the labels set by Vertex Core Infra itself. Tracking labels' keys are defined with special format: goog-[\\p{Ll}\\p{N}]+ E.g. \"key\": \"goog-k8s-cluster-name\",\"value\": \"us-east1-b4rk\"",
          "type": "object",
          "additionalProperties": {
            "type": "string"
          }
        },
        "machineSpec": {
          "type": "string",
          "description": "Required. Machine spec, e.g. N1_STANDARD_4.",
          "enumDescriptions": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
          ],
          "enum": [
            "UNKNOWN_MACHINE_SPEC",
            "N1_STANDARD_2",
            "N1_STANDARD_4",
            "N1_STANDARD_8",
            "N1_STANDARD_16",
            "N1_STANDARD_32",
            "N1_STANDARD_64",
            "N1_STANDARD_96",
            "N1_HIGHMEM_2",
            "N1_HIGHMEM_4",
            "N1_HIGHMEM_8",
            "N1_HIGHMEM_16",
            "N1_HIGHMEM_32",
            "N1_HIGHMEM_64",
            "N1_HIGHMEM_96",
            "N1_HIGHCPU_2",
            "N1_HIGHCPU_4",
            "N1_HIGHCPU_8",
            "N1_HIGHCPU_16",
            "N1_HIGHCPU_32",
            "N1_HIGHCPU_64",
            "N1_HIGHCPU_96",
            "A2_HIGHGPU_1G",
            "A2_HIGHGPU_2G",
            "A2_HIGHGPU_4G",
            "A2_HIGHGPU_8G",
            "A2_MEGAGPU_16G",
            "A2_ULTRAGPU_1G",
            "A2_ULTRAGPU_2G",
            "A2_ULTRAGPU_4G",
            "A2_ULTRAGPU_8G",
            "A3_HIGHGPU_8G",
            "E2_STANDARD_2",
            "E2_STANDARD_4",
            "E2_STANDARD_8",
            "E2_STANDARD_16",
            "E2_STANDARD_32",
            "E2_HIGHMEM_2",
            "E2_HIGHMEM_4",
            "E2_HIGHMEM_8",
            "E2_HIGHMEM_16",
            "E2_HIGHCPU_2",
            "E2_HIGHCPU_4",
            "E2_HIGHCPU_8",
            "E2_HIGHCPU_16",
            "E2_HIGHCPU_32",
            "N2_STANDARD_2",
            "N2_STANDARD_4",
            "N2_STANDARD_8",
            "N2_STANDARD_16",
            "N2_STANDARD_32",
            "N2_STANDARD_48",
            "N2_STANDARD_64",
            "N2_STANDARD_80",
            "N2_STANDARD_96",
            "N2_STANDARD_128",
            "N2_HIGHMEM_2",
            "N2_HIGHMEM_4",
            "N2_HIGHMEM_8",
            "N2_HIGHMEM_16",
            "N2_HIGHMEM_32",
            "N2_HIGHMEM_48",
            "N2_HIGHMEM_64",
            "N2_HIGHMEM_80",
            "N2_HIGHMEM_96",
            "N2_HIGHMEM_128",
            "N2_HIGHCPU_2",
            "N2_HIGHCPU_4",
            "N2_HIGHCPU_8",
            "N2_HIGHCPU_16",
            "N2_HIGHCPU_32",
            "N2_HIGHCPU_48",
            "N2_HIGHCPU_64",
            "N2_HIGHCPU_80",
            "N2_HIGHCPU_96",
            "N2D_STANDARD_2",
            "N2D_STANDARD_4",
            "N2D_STANDARD_8",
            "N2D_STANDARD_16",
            "N2D_STANDARD_32",
            "N2D_STANDARD_48",
            "N2D_STANDARD_64",
            "N2D_STANDARD_80",
            "N2D_STANDARD_96",
            "N2D_STANDARD_128",
            "N2D_STANDARD_224",
            "N2D_HIGHMEM_2",
            "N2D_HIGHMEM_4",
            "N2D_HIGHMEM_8",
            "N2D_HIGHMEM_16",
            "N2D_HIGHMEM_32",
            "N2D_HIGHMEM_48",
            "N2D_HIGHMEM_64",
            "N2D_HIGHMEM_80",
            "N2D_HIGHMEM_96",
            "N2D_HIGHCPU_2",
            "N2D_HIGHCPU_4",
            "N2D_HIGHCPU_8",
            "N2D_HIGHCPU_16",
            "N2D_HIGHCPU_32",
            "N2D_HIGHCPU_48",
            "N2D_HIGHCPU_64",
            "N2D_HIGHCPU_80",
            "N2D_HIGHCPU_96",
            "N2D_HIGHCPU_128",
            "N2D_HIGHCPU_224",
            "C2_STANDARD_4",
            "C2_STANDARD_8",
            "C2_STANDARD_16",
            "C2_STANDARD_30",
            "C2_STANDARD_60",
            "C2D_STANDARD_2",
            "C2D_STANDARD_4",
            "C2D_STANDARD_8",
            "C2D_STANDARD_16",
            "C2D_STANDARD_32",
            "C2D_STANDARD_56",
            "C2D_STANDARD_112",
            "C2D_HIGHCPU_2",
            "C2D_HIGHCPU_4",
            "C2D_HIGHCPU_8",
            "C2D_HIGHCPU_16",
            "C2D_HIGHCPU_32",
            "C2D_HIGHCPU_56",
            "C2D_HIGHCPU_112",
            "C2D_HIGHMEM_2",
            "C2D_HIGHMEM_4",
            "C2D_HIGHMEM_8",
            "C2D_HIGHMEM_16",
            "C2D_HIGHMEM_32",
            "C2D_HIGHMEM_56",
            "C2D_HIGHMEM_112",
            "G2_STANDARD_4",
            "G2_STANDARD_8",
            "G2_STANDARD_12",
            "G2_STANDARD_16",
            "G2_STANDARD_24",
            "G2_STANDARD_32",
            "G2_STANDARD_48",
            "G2_STANDARD_96",
            "C3_STANDARD_4",
            "C3_STANDARD_8",
            "C3_STANDARD_22",
            "C3_STANDARD_44",
            "C3_STANDARD_88",
            "C3_STANDARD_176",
            "C3_HIGHCPU_4",
            "C3_HIGHCPU_8",
            "C3_HIGHCPU_22",
            "C3_HIGHCPU_44",
            "C3_HIGHCPU_88",
            "C3_HIGHCPU_176",
            "C3_HIGHMEM_4",
            "C3_HIGHMEM_8",
            "C3_HIGHMEM_22",
            "C3_HIGHMEM_44",
            "C3_HIGHMEM_88",
            "C3_HIGHMEM_176"
          ]
        },
        "cpuType": {
          "description": "Required. Type of cpu, e.g. N2.",
          "type": "string",
          "enum": [
            "UNKNOWN_CPU_TYPE",
            "A2",
            "A3",
            "C2",
            "C2D",
            "CUSTOM",
            "E2",
            "G2",
            "C3",
            "M2",
            "M1",
            "N1",
            "N2_CUSTOM",
            "N2",
            "N2D"
          ],
          "enumDescriptions": [
            "",
            "GPU-based machine, skip quota reporting.",
            "GPU-based machine, skip quota reporting.",
            "COMPUTE_OPTIMIZED",
            "",
            "",
            "",
            "GPU-based machine, skip quota reporting.",
            "",
            "MEMORY_OPTIMIZED_UPGRADE_PREMIUM",
            "MEMORY_OPTIMIZED",
            "",
            "",
            "",
            ""
          ]
        },
        "coreSec": {
          "description": "Required. Total seconds of core usage, e.g. 4.",
          "type": "string",
          "format": "int64"
        },
        "coreNumber": {
          "type": "string",
          "description": "Required. Number of CPU cores.",
          "format": "int64"
        }
      },
      "description": "Metric for billing reports."
    },
    "XPSExportModelOutputConfig": {
      "type": "object",
      "properties": {
        "coreMlFormat": {
          "$ref": "XPSCoreMlFormat"
        },
        "tfSavedModelFormat": {
          "$ref": "XPSTfSavedModelFormat"
        },
        "outputGcsUri": {
          "description": "The Google Cloud Storage (GCS) directory where XPS will output the exported models and related files. Format: gs://bucket/directory",
          "type": "string"
        },
        "dockerFormat": {
          "$ref": "XPSDockerFormat"
        },
        "edgeTpuTfLiteFormat": {
          "$ref": "XPSEdgeTpuTfLiteFormat"
        },
        "outputGcrUri": {
          "type": "string",
          "description": "The Google Contained Registry (GCR) path the exported files to be pushed to. This location is set if the exported format is DOCKDER."
        },
        "exportFirebaseAuxiliaryInfo": {
          "type": "boolean",
          "description": "For any model and format: If true, will additionally export FirebaseExportedModelInfo in a firebase.txt file."
        },
        "tfLiteFormat": {
          "$ref": "XPSTfLiteFormat"
        },
        "tfJsFormat": {
          "$ref": "XPSTfJsFormat"
        }
      },
      "id": "XPSExportModelOutputConfig"
    },
    "XPSSpeechModelSpecSubModelSpec": {
      "id": "XPSSpeechModelSpecSubModelSpec",
      "properties": {
        "clientId": {
          "description": "In S3, Recognition ClientContextId.client_id",
          "type": "string"
        },
        "biasingModelType": {
          "description": "Type of the biasing model.",
          "enumDescriptions": [
            "",
            "Build biasing model on top of COMMAND_AND_SEARCH model",
            "Build biasing model on top of PHONE_CALL model",
            "Build biasing model on top of VIDEO model",
            "Build biasing model on top of DEFAULT model"
          ],
          "enum": [
            "BIASING_MODEL_TYPE_UNSPECIFIED",
            "COMMAND_AND_SEARCH",
            "PHONE_CALL",
            "VIDEO",
            "DEFAULT"
          ],
          "type": "string"
        },
        "contextId": {
          "type": "string",
          "description": "In S3, Recognition ClientContextId.context_id"
        },
        "isEnhancedModel": {
          "description": "If true then it means we have an enhanced version of the biasing models.",
          "type": "boolean"
        }
      },
      "type": "object"
    },
    "XPSCoreMlFormat": {
      "id": "XPSCoreMlFormat",
      "properties": {},
      "type": "object",
      "description": "A model format used for iOS mobile devices."
    },
    "XPSVideoExportModelSpec": {
      "description": "Information of downloadable models that are pre-generated as part of training flow and will be persisted in AutoMl backend. Upon receiving ExportModel request from user, AutoMl backend can serve the pre-generated models to user if exists (by copying the files from internal path to user provided location), otherwise, AutoMl backend will call xPS ExportModel API to generate the model on the fly with the requesting format.",
      "properties": {
        "exportModelOutputConfig": {
          "type": "array",
          "items": {
            "$ref": "XPSExportModelOutputConfig"
          },
          "description": "Contains the model format and internal location of the model files to be exported/downloaded. Use the GCS bucket name which is provided via TrainRequest.gcs_bucket_name to store the model files."
        }
      },
      "id": "XPSVideoExportModelSpec",
      "type": "object"
    },
    "Color": {
      "description": "Represents a color in the RGBA color space. This representation is designed for simplicity of conversion to and from color representations in various languages over compactness. For example, the fields of this representation can be trivially provided to the constructor of `java.awt.Color` in Java; it can also be trivially provided to UIColor's `+colorWithRed:green:blue:alpha` method in iOS; and, with just a little work, it can be easily formatted into a CSS `rgba()` string in JavaScript. This reference page doesn't have information about the absolute color space that should be used to interpret the RGB valuefor example, sRGB, Adobe RGB, DCI-P3, and BT.2020. By default, applications should assume the sRGB color space. When color equality needs to be decided, implementations, unless documented otherwise, treat two colors as equal if all their red, green, blue, and alpha values each differ by at most `1e-5`. Example (Java): import com.google.type.Color; // ... public static java.awt.Color fromProto(Color protocolor) { float alpha = protocolor.hasAlpha() ? protocolor.getAlpha().getValue() : 1.0; return new java.awt.Color( protocolor.getRed(), protocolor.getGreen(), protocolor.getBlue(), alpha); } public static Color toProto(java.awt.Color color) { float red = (float) color.getRed(); float green = (float) color.getGreen(); float blue = (float) color.getBlue(); float denominator = 255.0; Color.Builder resultBuilder = Color .newBuilder() .setRed(red / denominator) .setGreen(green / denominator) .setBlue(blue / denominator); int alpha = color.getAlpha(); if (alpha != 255) { result.setAlpha( FloatValue .newBuilder() .setValue(((float) alpha) / denominator) .build()); } return resultBuilder.build(); } // ... Example (iOS / Obj-C): // ... static UIColor* fromProto(Color* protocolor) { float red = [protocolor red]; float green = [protocolor green]; float blue = [protocolor blue]; FloatValue* alpha_wrapper = [protocolor alpha]; float alpha = 1.0; if (alpha_wrapper != nil) { alpha = [alpha_wrapper value]; } return [UIColor colorWithRed:red green:green blue:blue alpha:alpha]; } static Color* toProto(UIColor* color) { CGFloat red, green, blue, alpha; if (![color getRed:&red green:&green blue:&blue alpha:&alpha]) { return nil; } Color* result = [[Color alloc] init]; [result setRed:red]; [result setGreen:green]; [result setBlue:blue]; if (alpha \u003c= 0.9999) { [result setAlpha:floatWrapperWithValue(alpha)]; } [result autorelease]; return result; } // ... Example (JavaScript): // ... var protoToCssColor = function(rgb_color) { var redFrac = rgb_color.red || 0.0; var greenFrac = rgb_color.green || 0.0; var blueFrac = rgb_color.blue || 0.0; var red = Math.floor(redFrac * 255); var green = Math.floor(greenFrac * 255); var blue = Math.floor(blueFrac * 255); if (!('alpha' in rgb_color)) { return rgbToCssColor(red, green, blue); } var alphaFrac = rgb_color.alpha.value || 0.0; var rgbParams = [red, green, blue].join(','); return ['rgba(', rgbParams, ',', alphaFrac, ')'].join(''); }; var rgbToCssColor = function(red, green, blue) { var rgbNumber = new Number((red \u003c\u003c 16) | (green \u003c\u003c 8) | blue); var hexString = rgbNumber.toString(16); var missingZeros = 6 - hexString.length; var resultBuilder = ['#']; for (var i = 0; i \u003c missingZeros; i++) { resultBuilder.push('0'); } resultBuilder.push(hexString); return resultBuilder.join(''); }; // ...",
      "id": "Color",
      "properties": {
        "green": {
          "type": "number",
          "format": "float",
          "description": "The amount of green in the color as a value in the interval [0, 1]."
        },
        "blue": {
          "description": "The amount of blue in the color as a value in the interval [0, 1].",
          "format": "float",
          "type": "number"
        },
        "red": {
          "description": "The amount of red in the color as a value in the interval [0, 1].",
          "format": "float",
          "type": "number"
        },
        "alpha": {
          "description": "The fraction of this color that should be applied to the pixel. That is, the final pixel color is defined by the equation: `pixel color = alpha * (this color) + (1.0 - alpha) * (background color)` This means that a value of 1.0 corresponds to a solid color, whereas a value of 0.0 corresponds to a completely transparent color. This uses a wrapper message rather than a simple float scalar so that it is possible to distinguish between a default value and the value being unset. If omitted, this color object is rendered as a solid color (as if the alpha value had been explicitly given a value of 1.0).",
          "format": "float",
          "type": "number"
        }
      },
      "type": "object"
    },
    "XPSTablesRegressionMetrics": {
      "type": "object",
      "description": "Metrics for Tables regression problems.",
      "id": "XPSTablesRegressionMetrics",
      "properties": {
        "regressionMetricsEntries": {
          "type": "array",
          "items": {
            "$ref": "XPSRegressionMetricsEntry"
          },
          "description": "A list of actual versus predicted points for the model being evaluated."
        },
        "rSquared": {
          "format": "double",
          "description": "R squared.",
          "type": "number"
        },
        "rootMeanSquaredLogError": {
          "description": "Root mean squared log error.",
          "format": "double",
          "type": "number"
        },
        "rootMeanSquaredError": {
          "description": "Root mean squared error.",
          "type": "number",
          "format": "double"
        },
        "meanAbsoluteError": {
          "description": "Mean absolute error.",
          "format": "double",
          "type": "number"
        },
        "meanAbsolutePercentageError": {
          "type": "number",
          "format": "double",
          "description": "Mean absolute percentage error, only set if all of the target column's values are positive."
        }
      }
    },
    "XPSColumnSpecCorrelatedColumn": {
      "properties": {
        "correlationStats": {
          "$ref": "XPSCorrelationStats"
        },
        "columnId": {
          "format": "int32",
          "type": "integer"
        }
      },
      "id": "XPSColumnSpecCorrelatedColumn",
      "type": "object",
      "description": "Identifies a table's column, and its correlation with the column this ColumnSpec describes."
    },
    "XPSTrackMetricsEntryConfidenceMetricsEntry": {
      "id": "XPSTrackMetricsEntryConfidenceMetricsEntry",
      "properties": {
        "trackingPrecision": {
          "format": "float",
          "description": "Output only. Tracking precision.",
          "type": "number"
        },
        "boundingBoxIou": {
          "description": "Output only. Bounding box intersection-over-union precision. Measures how well the bounding boxes overlap between each other (e.g. complete overlap or just barely above iou_threshold).",
          "type": "number",
          "format": "float"
        },
        "trackingRecall": {
          "type": "number",
          "description": "Output only. Tracking recall.",
          "format": "float"
        },
        "mismatchRate": {
          "description": "Output only. Mismatch rate, which measures the tracking consistency, i.e. correctness of instance ID continuity.",
          "format": "float",
          "type": "number"
        },
        "confidenceThreshold": {
          "format": "float",
          "description": "Output only. The confidence threshold value used to compute the metrics.",
          "type": "number"
        }
      },
      "type": "object",
      "description": "Metrics for a single confidence threshold. Next tag: 6."
    },
    "ModerateTextResponse": {
      "properties": {
        "moderationCategories": {
          "type": "array",
          "items": {
            "$ref": "ClassificationCategory"
          },
          "description": "Harmful and sensitive categories representing the input document."
        }
      },
      "type": "object",
      "id": "ModerateTextResponse",
      "description": "The document moderation response message."
    },
    "AnalyzeSentimentResponse": {
      "id": "AnalyzeSentimentResponse",
      "properties": {
        "documentSentiment": {
          "$ref": "Sentiment",
          "description": "The overall sentiment of the input document."
        },
        "language": {
          "description": "The language of the text, which will be the same as the language specified in the request or, if not specified, the automatically-detected language. See Document.language field for more details.",
          "type": "string"
        },
        "sentences": {
          "description": "The sentiment for all the sentences in the document.",
          "type": "array",
          "items": {
            "$ref": "Sentence"
          }
        }
      },
      "type": "object",
      "description": "The sentiment analysis response message."
    },
    "XPSTfLiteFormat": {
      "description": "LINT.IfChange A model format used for mobile and IoT devices. See https://www.tensorflow.org/lite.",
      "id": "XPSTfLiteFormat",
      "type": "object",
      "properties": {}
    },
    "XPSTextComponentModel": {
      "description": "Component model. Next ID: 10",
      "properties": {
        "submodelName": {
          "type": "string",
          "description": "The name of the trained NL submodel."
        },
        "submodelType": {
          "type": "string",
          "enum": [
            "TEXT_MODEL_TYPE_UNSPECIFIED",
            "TEXT_MODEL_TYPE_DEFAULT",
            "TEXT_MODEL_TYPE_META_ARCHITECT",
            "TEXT_MODEL_TYPE_ATC",
            "TEXT_MODEL_TYPE_CLARA2",
            "TEXT_MODEL_TYPE_CHATBASE",
            "TEXT_MODEL_TYPE_SAFT_SPAN_LABELING",
            "TEXT_MODEL_TYPE_TEXT_EXTRACTION",
            "TEXT_MODEL_TYPE_RELATIONSHIP_EXTRACTION",
            "TEXT_MODEL_TYPE_COMPOSITE",
            "TEXT_MODEL_TYPE_ALL_MODELS",
            "TEXT_MODEL_TYPE_BERT",
            "TEXT_MODEL_TYPE_ENC_PALM"
          ],
          "enumDescriptions": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "Model type for entity extraction.",
            "Model type for relationship extraction.",
            "A composite model represents a set of component models that have to be used together for prediction. A composite model appears to be a single model to the model user. It may contain only one component model. Please refer to go/cnl-composite-models for more information.",
            "Model type used to train default, MA, and ATC models in a single batch worker pipeline.",
            "BERT pipeline needs a specific model type, since it uses a different TFX configuration compared with DEFAULT (despite sharing most of the code).",
            "Model type for EncPaLM."
          ],
          "description": "The type of trained NL submodel"
        },
        "servoModelName": {
          "description": "The name of servo model. Populated by uCAIP BE as part of online PredictRequest.",
          "type": "string"
        },
        "versionNumber": {
          "format": "int64",
          "type": "string",
          "description": "The servomatic model version number. Populated by uCAIP BE as part of online PredictRequest."
        },
        "batchPredictionModelGcsUri": {
          "type": "string",
          "description": "The Cloud Storage resource path to hold batch prediction model."
        },
        "onlinePredictionModelGcsUri": {
          "type": "string",
          "description": "The Cloud Storage resource path to hold online prediction model."
        },
        "partition": {
          "enum": [
            "PARTITION_TYPE_UNSPECIFIED",
            "PARTITION_ZERO",
            "PARTITION_REDUCED_HOMING",
            "PARTITION_JELLYFISH",
            "PARTITION_CPU",
            "PARTITION_CUSTOM_STORAGE_CPU"
          ],
          "type": "string",
          "description": "The partition where the model is deployed. Populated by uCAIP BE as part of online PredictRequest.",
          "enumDescriptions": [
            "",
            "The default partition.",
            "It has significantly lower replication than partition-0 and is located in the US only. It also has a larger model size limit and higher default RAM quota than partition-0. Customers with batch traffic, US-based traffic, or very large models should use this partition. Capacity in this partition is significantly cheaper than partition-0.",
            "To be used by customers with Jellyfish-accelerated ops. See go/servomatic-jellyfish for details.",
            "The partition used by regionalized servomatic cloud regions.",
            "The partition used for loading models from custom storage."
          ]
        },
        "servingArtifact": {
          "description": "The default model binary file used for serving (e.g. online predict, batch predict) via public Cloud Ai Platform API.",
          "$ref": "XPSModelArtifactItem"
        },
        "tfRuntimeVersion": {
          "type": "string",
          "description": "## The fields below are only populated under uCAIP request scope. https://cloud.google.com/ml-engine/docs/runtime-version-list"
        }
      },
      "id": "XPSTextComponentModel",
      "type": "object"
    },
    "Sentiment": {
      "description": "Represents the feeling associated with the entire text or entities in the text. Next ID: 6",
      "type": "object",
      "id": "Sentiment",
      "properties": {
        "magnitude": {
          "format": "float",
          "description": "A non-negative number in the [0, +inf) range, which represents the absolute magnitude of sentiment regardless of score (positive or negative).",
          "type": "number"
        },
        "score": {
          "type": "number",
          "format": "float",
          "description": "Sentiment score between -1.0 (negative sentiment) and 1.0 (positive sentiment)."
        }
      }
    },
    "XPSTablesClassificationMetricsCurveMetrics": {
      "description": "Metrics curve data point for a single value.",
      "properties": {
        "logLoss": {
          "description": "The Log loss metric.",
          "type": "number",
          "format": "double"
        },
        "aucPr": {
          "type": "number",
          "format": "double",
          "description": "The area under the precision-recall curve."
        },
        "confidenceMetricsEntries": {
          "type": "array",
          "description": "Metrics that have confidence thresholds. Precision-recall curve and ROC curve can be derived from them.",
          "items": {
            "$ref": "XPSTablesConfidenceMetricsEntry"
          }
        },
        "positionThreshold": {
          "format": "int32",
          "description": "The position threshold value used to compute the metrics.",
          "type": "integer"
        },
        "value": {
          "description": "The CATEGORY row value (for ARRAY unnested) the curve metrics are for.",
          "type": "string"
        },
        "aucRoc": {
          "format": "double",
          "description": "The area under receiver operating characteristic curve.",
          "type": "number"
        }
      },
      "type": "object",
      "id": "XPSTablesClassificationMetricsCurveMetrics"
    },
    "XPSColorMapIntColor": {
      "id": "XPSColorMapIntColor",
      "type": "object",
      "description": "RGB color and each channel is represented by an integer.",
      "properties": {
        "blue": {
          "format": "int32",
          "description": "The value should be in range of [0, 255].",
          "type": "integer"
        },
        "green": {
          "type": "integer",
          "format": "int32",
          "description": "The value should be in range of [0, 255]."
        },
        "red": {
          "type": "integer",
          "description": "The value should be in range of [0, 255].",
          "format": "int32"
        }
      }
    },
    "ClassificationCategory": {
      "description": "Represents a category returned from the text classifier.",
      "id": "ClassificationCategory",
      "properties": {
        "confidence": {
          "format": "float",
          "description": "The classifier's confidence of the category. Number represents how certain the classifier is that this category represents the given text.",
          "type": "number"
        },
        "name": {
          "type": "string",
          "description": "The name of the category representing the document."
        }
      },
      "type": "object"
    },
    "XPSTranslationEvaluationMetrics": {
      "description": "Evaluation metrics for the dataset.",
      "properties": {
        "baseBleuScore": {
          "format": "double",
          "description": "BLEU score for base model.",
          "type": "number"
        },
        "bleuScore": {
          "description": "BLEU score.",
          "type": "number",
          "format": "double"
        }
      },
      "type": "object",
      "id": "XPSTranslationEvaluationMetrics"
    },
    "XPSColumnSpec": {
      "properties": {
        "forecastingMetadata": {
          "$ref": "XPSColumnSpecForecastingMetadata"
        },
        "dataType": {
          "description": "The data type of the column. It's outputed in Preprocess rpc and a required input for RefreshTablesStats and Train.",
          "$ref": "XPSDataType"
        },
        "topCorrelatedColumns": {
          "type": "array",
          "description": "It's outputed in RefreshTablesStats, and a required input in Train.",
          "items": {
            "$ref": "XPSColumnSpecCorrelatedColumn"
          }
        },
        "columnId": {
          "format": "int32",
          "description": "The unique id of the column. When Preprocess, the Tables BE will popuate the order id of the column, which reflects the order of the column inside the table, i.e. 0 means the first column in the table, N-1 means the last column. AutoML BE will persist this order id in Spanner and set the order id here when calling RefreshTablesStats and Train. Note: it's different than the column_spec_id that is generated in AutoML BE.",
          "type": "integer"
        },
        "dataStats": {
          "$ref": "XPSDataStats",
          "description": "The data stats of the column. It's outputed in RefreshTablesStats and a required input for Train."
        },
        "displayName": {
          "description": "The display name of the column. It's outputed in Preprocess and a required input for RefreshTablesStats and Train.",
          "type": "string"
        }
      },
      "type": "object",
      "id": "XPSColumnSpec"
    },
    "XPSResponseExplanationMetadataOutputMetadata": {
      "description": "Metadata of the prediction output to be explained.",
      "id": "XPSResponseExplanationMetadataOutputMetadata",
      "type": "object",
      "properties": {
        "outputTensorName": {
          "description": "Name of the output tensor. Only needed in train response.",
          "type": "string"
        }
      }
    },
    "ClassificationModelOptions": {
      "id": "ClassificationModelOptions",
      "type": "object",
      "properties": {
        "v2Model": {
          "$ref": "ClassificationModelOptionsV2Model",
          "description": "Setting this field will use the V2 model with the appropriate content categories version. The V2 model is a better performing model."
        },
        "v1Model": {
          "$ref": "ClassificationModelOptionsV1Model",
          "description": "Setting this field will use the V1 model and V1 content categories version. The V1 model is a legacy model; support for this will be discontinued in the future."
        }
      },
      "description": "Model options available for classification requests."
    },
    "XPSIntegratedGradientsAttribution": {
      "id": "XPSIntegratedGradientsAttribution",
      "description": "An attribution method that computes the Aumann-Shapley value taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1703.01365",
      "deprecated": true,
      "properties": {
        "stepCount": {
          "type": "integer",
          "format": "int32",
          "description": "The number of steps for approximating the path integral. A good value to start is 50 and gradually increase until the sum to diff property is within the desired error range. Valid range of its value is [1, 100], inclusively."
        }
      },
      "type": "object"
    },
    "XPSDataStats": {
      "id": "XPSDataStats",
      "properties": {
        "categoryStats": {
          "description": "The statistics for CATEGORY DataType.",
          "$ref": "XPSCategoryStats"
        },
        "float64Stats": {
          "description": "The statistics for FLOAT64 DataType.",
          "$ref": "XPSFloat64Stats"
        },
        "structStats": {
          "$ref": "XPSStructStats",
          "description": "The statistics for STRUCT DataType."
        },
        "validValueCount": {
          "format": "int64",
          "description": "The number of values that are valid.",
          "type": "string"
        },
        "arrayStats": {
          "description": "The statistics for ARRAY DataType.",
          "$ref": "XPSArrayStats"
        },
        "nullValueCount": {
          "format": "int64",
          "type": "string",
          "description": "The number of values that are null."
        },
        "timestampStats": {
          "$ref": "XPSTimestampStats",
          "description": "The statistics for TIMESTAMP DataType."
        },
        "stringStats": {
          "description": "The statistics for STRING DataType.",
          "$ref": "XPSStringStats"
        },
        "distinctValueCount": {
          "type": "string",
          "format": "int64",
          "description": "The number of distinct values."
        }
      },
      "type": "object",
      "description": "The data statistics of a series of values that share the same DataType."
    },
    "Status": {
      "id": "Status",
      "type": "object",
      "properties": {
        "details": {
          "items": {
            "additionalProperties": {
              "description": "Properties of the object. Contains field @type with type URL.",
              "type": "any"
            },
            "type": "object"
          },
          "type": "array",
          "description": "A list of messages that carry the error details. There is a common set of message types for APIs to use."
        },
        "message": {
          "description": "A developer-facing error message, which should be in English. Any user-facing error message should be localized and sent in the google.rpc.Status.details field, or localized by the client.",
          "type": "string"
        },
        "code": {
          "type": "integer",
          "format": "int32",
          "description": "The status code, which should be an enum value of google.rpc.Code."
        }
      },
      "description": "The `Status` type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by [gRPC](https://github.com/grpc). Each `Status` message contains three pieces of data: error code, error message, and error details. You can find out more about this error model and how to work with it in the [API Design Guide](https://cloud.google.com/apis/design/errors)."
    },
    "XPSBoundingBoxMetricsEntryConfidenceMetricsEntry": {
      "properties": {
        "f1Score": {
          "description": "The harmonic mean of recall and precision.",
          "type": "number",
          "format": "float"
        },
        "recall": {
          "format": "float",
          "type": "number",
          "description": "Recall for the given confidence threshold."
        },
        "confidenceThreshold": {
          "description": "The confidence threshold value used to compute the metrics.",
          "format": "float",
          "type": "number"
        },
        "precision": {
          "type": "number",
          "format": "float",
          "description": "Precision for the given confidence threshold."
        }
      },
      "id": "XPSBoundingBoxMetricsEntryConfidenceMetricsEntry",
      "description": "Metrics for a single confidence threshold.",
      "type": "object"
    },
    "XPSRow": {
      "properties": {
        "values": {
          "items": {
            "type": "any"
          },
          "type": "array",
          "description": "The values of the row cells, given in the same order as the column_ids. If column_ids is not set, then in the same order as the input_feature_column_ids in TablesModelMetadata."
        },
        "columnIds": {
          "description": "The ids of the columns. Note: The below `values` field must match order of this field, if this field is set.",
          "type": "array",
          "items": {
            "format": "int32",
            "type": "integer"
          }
        }
      },
      "id": "XPSRow",
      "type": "object"
    },
    "AnalyzeSyntaxResponse": {
      "properties": {
        "tokens": {
          "description": "Tokens, along with their syntactic information, in the input document.",
          "type": "array",
          "items": {
            "$ref": "Token"
          }
        },
        "sentences": {
          "items": {
            "$ref": "Sentence"
          },
          "description": "Sentences in the input document.",
          "type": "array"
        },
        "language": {
          "type": "string",
          "description": "The language of the text, which will be the same as the language specified in the request or, if not specified, the automatically-detected language. See Document.language field for more details."
        }
      },
      "type": "object",
      "description": "The syntax analysis response message.",
      "id": "AnalyzeSyntaxResponse"
    },
    "GpuMetric": {
      "properties": {
        "machineSpec": {
          "type": "string",
          "enumDescriptions": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
          ],
          "description": "Required. Machine spec, e.g. N1_STANDARD_4.",
          "enum": [
            "UNKNOWN_MACHINE_SPEC",
            "N1_STANDARD_2",
            "N1_STANDARD_4",
            "N1_STANDARD_8",
            "N1_STANDARD_16",
            "N1_STANDARD_32",
            "N1_STANDARD_64",
            "N1_STANDARD_96",
            "N1_HIGHMEM_2",
            "N1_HIGHMEM_4",
            "N1_HIGHMEM_8",
            "N1_HIGHMEM_16",
            "N1_HIGHMEM_32",
            "N1_HIGHMEM_64",
            "N1_HIGHMEM_96",
            "N1_HIGHCPU_2",
            "N1_HIGHCPU_4",
            "N1_HIGHCPU_8",
            "N1_HIGHCPU_16",
            "N1_HIGHCPU_32",
            "N1_HIGHCPU_64",
            "N1_HIGHCPU_96",
            "A2_HIGHGPU_1G",
            "A2_HIGHGPU_2G",
            "A2_HIGHGPU_4G",
            "A2_HIGHGPU_8G",
            "A2_MEGAGPU_16G",
            "A2_ULTRAGPU_1G",
            "A2_ULTRAGPU_2G",
            "A2_ULTRAGPU_4G",
            "A2_ULTRAGPU_8G",
            "A3_HIGHGPU_8G",
            "E2_STANDARD_2",
            "E2_STANDARD_4",
            "E2_STANDARD_8",
            "E2_STANDARD_16",
            "E2_STANDARD_32",
            "E2_HIGHMEM_2",
            "E2_HIGHMEM_4",
            "E2_HIGHMEM_8",
            "E2_HIGHMEM_16",
            "E2_HIGHCPU_2",
            "E2_HIGHCPU_4",
            "E2_HIGHCPU_8",
            "E2_HIGHCPU_16",
            "E2_HIGHCPU_32",
            "N2_STANDARD_2",
            "N2_STANDARD_4",
            "N2_STANDARD_8",
            "N2_STANDARD_16",
            "N2_STANDARD_32",
            "N2_STANDARD_48",
            "N2_STANDARD_64",
            "N2_STANDARD_80",
            "N2_STANDARD_96",
            "N2_STANDARD_128",
            "N2_HIGHMEM_2",
            "N2_HIGHMEM_4",
            "N2_HIGHMEM_8",
            "N2_HIGHMEM_16",
            "N2_HIGHMEM_32",
            "N2_HIGHMEM_48",
            "N2_HIGHMEM_64",
            "N2_HIGHMEM_80",
            "N2_HIGHMEM_96",
            "N2_HIGHMEM_128",
            "N2_HIGHCPU_2",
            "N2_HIGHCPU_4",
            "N2_HIGHCPU_8",
            "N2_HIGHCPU_16",
            "N2_HIGHCPU_32",
            "N2_HIGHCPU_48",
            "N2_HIGHCPU_64",
            "N2_HIGHCPU_80",
            "N2_HIGHCPU_96",
            "N2D_STANDARD_2",
            "N2D_STANDARD_4",
            "N2D_STANDARD_8",
            "N2D_STANDARD_16",
            "N2D_STANDARD_32",
            "N2D_STANDARD_48",
            "N2D_STANDARD_64",
            "N2D_STANDARD_80",
            "N2D_STANDARD_96",
            "N2D_STANDARD_128",
            "N2D_STANDARD_224",
            "N2D_HIGHMEM_2",
            "N2D_HIGHMEM_4",
            "N2D_HIGHMEM_8",
            "N2D_HIGHMEM_16",
            "N2D_HIGHMEM_32",
            "N2D_HIGHMEM_48",
            "N2D_HIGHMEM_64",
            "N2D_HIGHMEM_80",
            "N2D_HIGHMEM_96",
            "N2D_HIGHCPU_2",
            "N2D_HIGHCPU_4",
            "N2D_HIGHCPU_8",
            "N2D_HIGHCPU_16",
            "N2D_HIGHCPU_32",
            "N2D_HIGHCPU_48",
            "N2D_HIGHCPU_64",
            "N2D_HIGHCPU_80",
            "N2D_HIGHCPU_96",
            "N2D_HIGHCPU_128",
            "N2D_HIGHCPU_224",
            "C2_STANDARD_4",
            "C2_STANDARD_8",
            "C2_STANDARD_16",
            "C2_STANDARD_30",
            "C2_STANDARD_60",
            "C2D_STANDARD_2",
            "C2D_STANDARD_4",
            "C2D_STANDARD_8",
            "C2D_STANDARD_16",
            "C2D_STANDARD_32",
            "C2D_STANDARD_56",
            "C2D_STANDARD_112",
            "C2D_HIGHCPU_2",
            "C2D_HIGHCPU_4",
            "C2D_HIGHCPU_8",
            "C2D_HIGHCPU_16",
            "C2D_HIGHCPU_32",
            "C2D_HIGHCPU_56",
            "C2D_HIGHCPU_112",
            "C2D_HIGHMEM_2",
            "C2D_HIGHMEM_4",
            "C2D_HIGHMEM_8",
            "C2D_HIGHMEM_16",
            "C2D_HIGHMEM_32",
            "C2D_HIGHMEM_56",
            "C2D_HIGHMEM_112",
            "G2_STANDARD_4",
            "G2_STANDARD_8",
            "G2_STANDARD_12",
            "G2_STANDARD_16",
            "G2_STANDARD_24",
            "G2_STANDARD_32",
            "G2_STANDARD_48",
            "G2_STANDARD_96",
            "C3_STANDARD_4",
            "C3_STANDARD_8",
            "C3_STANDARD_22",
            "C3_STANDARD_44",
            "C3_STANDARD_88",
            "C3_STANDARD_176",
            "C3_HIGHCPU_4",
            "C3_HIGHCPU_8",
            "C3_HIGHCPU_22",
            "C3_HIGHCPU_44",
            "C3_HIGHCPU_88",
            "C3_HIGHCPU_176",
            "C3_HIGHMEM_4",
            "C3_HIGHMEM_8",
            "C3_HIGHMEM_22",
            "C3_HIGHMEM_44",
            "C3_HIGHMEM_88",
            "C3_HIGHMEM_176"
          ]
        },
        "gpuType": {
          "description": "Required. Type of GPU, e.g. NVIDIA_TESLA_V100.",
          "type": "string",
          "enumDescriptions": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
          ],
          "enum": [
            "UNKNOWN_GPU_TYPE",
            "NVIDIA_TESLA_A100",
            "NVIDIA_A100_80GB",
            "NVIDIA_TESLA_K80",
            "NVIDIA_L4",
            "NVIDIA_TESLA_P100",
            "NVIDIA_TESLA_P4",
            "NVIDIA_TESLA_T4",
            "NVIDIA_TESLA_V100",
            "NVIDIA_H100_80GB"
          ]
        },
        "gpuSec": {
          "type": "string",
          "format": "int64",
          "description": "Required. Seconds of GPU usage, e.g. 3600."
        },
        "trackingLabels": {
          "type": "object",
          "description": "Billing tracking labels. They do not contain any user data but only the labels set by Vertex Core Infra itself. Tracking labels' keys are defined with special format: goog-[\\p{Ll}\\p{N}]+ E.g. \"key\": \"goog-k8s-cluster-name\",\"value\": \"us-east1-b4rk\"",
          "additionalProperties": {
            "type": "string"
          }
        }
      },
      "id": "GpuMetric",
      "type": "object"
    },
    "XPSVideoBatchPredictOperationMetadata": {
      "type": "object",
      "id": "XPSVideoBatchPredictOperationMetadata",
      "properties": {
        "outputExamples": {
          "items": {
            "type": "string"
          },
          "type": "array",
          "description": "All the partial batch prediction results that are completed at the moment. Output examples are sorted by completion time. The order will not be changed. Each output example should be the path of a single RecordIO file of AnnotatedExamples."
        }
      }
    },
    "XPSTablesModelStructureModelParameters": {
      "description": "Model hyper-parameters for a model.",
      "id": "XPSTablesModelStructureModelParameters",
      "type": "object",
      "properties": {
        "hyperparameters": {
          "type": "array",
          "items": {
            "$ref": "XPSTablesModelStructureModelParametersParameter"
          }
        }
      }
    },
    "XPSResponseExplanationMetadataInputMetadata": {
      "type": "object",
      "description": "Metadata of the input of a feature.",
      "properties": {
        "inputTensorName": {
          "type": "string",
          "description": "Name of the input tensor for this model. Only needed in train response."
        },
        "modality": {
          "enum": [
            "MODALITY_UNSPECIFIED",
            "NUMERIC",
            "IMAGE",
            "CATEGORICAL"
          ],
          "description": "Modality of the feature. Valid values are: numeric, image. Defaults to numeric.",
          "type": "string",
          "enumDescriptions": [
            "",
            "",
            "",
            ""
          ]
        },
        "visualizationConfig": {
          "$ref": "XPSVisualization",
          "description": "Visualization configurations for image explanation."
        }
      },
      "id": "XPSResponseExplanationMetadataInputMetadata"
    },
    "XPSTextSentimentEvaluationMetrics": {
      "id": "XPSTextSentimentEvaluationMetrics",
      "type": "object",
      "description": "Model evaluation metrics for text sentiment problems.",
      "properties": {
        "recall": {
          "type": "number",
          "description": "Output only. Recall.",
          "format": "float"
        },
        "quadraticKappa": {
          "format": "float",
          "type": "number",
          "description": "Output only. Quadratic weighted kappa. Only set for the overall model evaluation, not for evaluation of a single annotation spec."
        },
        "meanSquaredError": {
          "format": "float",
          "description": "Output only. Mean squared error. Only set for the overall model evaluation, not for evaluation of a single annotation spec.",
          "type": "number"
        },
        "meanAbsoluteError": {
          "description": "Output only. Mean absolute error. Only set for the overall model evaluation, not for evaluation of a single annotation spec.",
          "type": "number",
          "format": "float"
        },
        "precision": {
          "format": "float",
          "type": "number",
          "description": "Output only. Precision."
        },
        "f1Score": {
          "type": "number",
          "format": "float",
          "description": "Output only. The harmonic mean of recall and precision."
        },
        "confusionMatrix": {
          "$ref": "XPSConfusionMatrix",
          "description": "Output only. Confusion matrix of the evaluation. Only set for the overall model evaluation, not for evaluation of a single annotation spec."
        },
        "linearKappa": {
          "format": "float",
          "type": "number",
          "description": "Output only. Linear weighted kappa. Only set for the overall model evaluation, not for evaluation of a single annotation spec."
        }
      }
    },
    "XPSBatchPredictResponse": {
      "type": "object",
      "id": "XPSBatchPredictResponse",
      "properties": {
        "exampleSet": {
          "$ref": "XPSExampleSet",
          "description": "Examples for batch prediction result. Under full API implementation, results are stored in shared RecordIO of AnnotatedExample protobufs, the annotations field of which is populated by XPS backend."
        }
      }
    },
    "XPSVisualization": {
      "description": "Visualization configurations for image explanation.",
      "deprecated": true,
      "id": "XPSVisualization",
      "type": "object",
      "properties": {
        "colorMap": {
          "description": "The color scheme used for the highlighted areas. Defaults to PINK_GREEN for Integrated Gradients attribution, which shows positive attributions in green and negative in pink. Defaults to VIRIDIS for XRAI attribution, which highlights the most influential regions in yellow and the least influential in blue.",
          "enumDescriptions": [
            "Should not be used.",
            "Positive: green. Negative: pink.",
            "Viridis color map: A perceptually uniform color mapping which is easier to see by those with colorblindness and progresses from yellow to green to blue. Positive: yellow. Negative: blue.",
            "Positive: red. Negative: red.",
            "Positive: green. Negative: green.",
            "Positive: green. Negative: red.",
            "PiYG palette."
          ],
          "enum": [
            "COLOR_MAP_UNSPECIFIED",
            "PINK_GREEN",
            "VIRIDIS",
            "RED",
            "GREEN",
            "RED_GREEN",
            "PINK_WHITE_GREEN"
          ],
          "type": "string"
        },
        "type": {
          "type": "string",
          "enumDescriptions": [
            "Should not be used.",
            "Shows which pixel contributed to the image prediction.",
            "Shows which region contributed to the image prediction by outlining the region."
          ],
          "enum": [
            "TYPE_UNSPECIFIED",
            "PIXELS",
            "OUTLINES"
          ],
          "description": "Type of the image visualization. Only applicable to Integrated Gradients attribution. OUTLINES shows regions of attribution, while PIXELS shows per-pixel attribution. Defaults to OUTLINES."
        },
        "clipPercentLowerbound": {
          "format": "float",
          "description": "Excludes attributions below the specified percentile, from the highlighted areas. Defaults to 62.",
          "type": "number"
        },
        "polarity": {
          "enumDescriptions": [
            "Default value. This is the same as POSITIVE.",
            "Highlights the pixels/outlines that were most influential to the model's prediction.",
            "Setting polarity to negative highlights areas that does not lead to the models's current prediction.",
            "Shows both positive and negative attributions."
          ],
          "enum": [
            "POLARITY_UNSPECIFIED",
            "POSITIVE",
            "NEGATIVE",
            "BOTH"
          ],
          "description": "Whether to only highlight pixels with positive contributions, negative or both. Defaults to POSITIVE.",
          "type": "string"
        },
        "clipPercentUpperbound": {
          "format": "float",
          "description": "Excludes attributions above the specified percentile from the highlighted areas. Using the clip_percent_upperbound and clip_percent_lowerbound together can be useful for filtering out noise and making it easier to see areas of strong attribution. Defaults to 99.9.",
          "type": "number"
        },
        "overlayType": {
          "enumDescriptions": [
            "Default value. This is the same as NONE.",
            "No overlay.",
            "The attributions are shown on top of the original image.",
            "The attributions are shown on top of grayscaled version of the original image.",
            "The attributions are used as a mask to reveal predictive parts of the image and hide the un-predictive parts."
          ],
          "type": "string",
          "description": "How the original image is displayed in the visualization. Adjusting the overlay can help increase visual clarity if the original image makes it difficult to view the visualization. Defaults to NONE.",
          "enum": [
            "OVERLAY_TYPE_UNSPECIFIED",
            "NONE",
            "ORIGINAL",
            "GRAYSCALE",
            "MASK_BLACK"
          ]
        }
      }
    },
    "XPSVideoActionRecognitionTrainResponse": {
      "type": "object",
      "properties": {
        "trainCostNodeSeconds": {
          "format": "int64",
          "description": "The actual train cost of creating this model, expressed in node seconds, i.e. 3,600 value in this field means 1 node hour.",
          "type": "string"
        },
        "modelArtifactSpec": {
          "$ref": "XPSVideoModelArtifactSpec",
          "description": "## The fields below are only populated under uCAIP request scope."
        }
      },
      "id": "XPSVideoActionRecognitionTrainResponse"
    },
    "XPSConfusionMatrix": {
      "id": "XPSConfusionMatrix",
      "properties": {
        "row": {
          "items": {
            "$ref": "XPSConfusionMatrixRow"
          },
          "type": "array",
          "description": "Rows in the confusion matrix. The number of rows is equal to the size of `annotation_spec_id_token`. `row[i].value[j]` is the number of examples that have ground truth of the `annotation_spec_id_token[i]` and are predicted as `annotation_spec_id_token[j]` by the model being evaluated."
        },
        "annotationSpecIdToken": {
          "description": "For the following three repeated fields, only one is intended to be set. annotation_spec_id_token is preferable to be set. ID tokens of the annotation specs used in the confusion matrix.",
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "sentimentLabel": {
          "items": {
            "format": "int32",
            "type": "integer"
          },
          "type": "array",
          "description": "Sentiment labels used in the confusion matrix. Set only for text sentiment models. For AutoML Text Revamp, use `annotation_spec_id_token` instead and leave this field empty."
        },
        "category": {
          "type": "array",
          "description": "Category (mainly for segmentation). Set only for image segmentation models. Note: uCAIP Image Segmentation should use annotation_spec_id_token.",
          "items": {
            "format": "int32",
            "type": "integer"
          }
        }
      },
      "description": "Confusion matrix of the model running the classification.",
      "type": "object"
    },
    "XPSImageClassificationTrainResponse": {
      "properties": {
        "modelServingSpec": {
          "$ref": "XPSImageModelServingSpec"
        },
        "classCount": {
          "format": "int64",
          "type": "string",
          "description": "Total number of classes."
        },
        "stopReason": {
          "description": "Stop reason for training job, e.g. 'TRAIN_BUDGET_REACHED', 'MODEL_CONVERGED', 'MODEL_EARLY_STOPPED'.",
          "type": "string",
          "enumDescriptions": [
            "",
            "",
            "Model fully converged, can not be resumbed training.",
            "Model early converged, can be further trained till full convergency."
          ],
          "enum": [
            "TRAIN_STOP_REASON_UNSPECIFIED",
            "TRAIN_STOP_REASON_BUDGET_REACHED",
            "TRAIN_STOP_REASON_MODEL_CONVERGED",
            "TRAIN_STOP_REASON_MODEL_EARLY_STOPPED"
          ]
        },
        "modelArtifactSpec": {
          "description": "## The fields below are only populated under uCAIP request scope.",
          "$ref": "XPSImageModelArtifactSpec"
        },
        "trainCostInNodeTime": {
          "description": "The actual cost to create this model. - For edge type model, the cost is expressed in node hour. - For cloud type model,the cost is expressed in compute hour. - Populated for models created before GA. To be deprecated after GA.",
          "format": "google-duration",
          "type": "string"
        },
        "trainCostNodeSeconds": {
          "description": "The actual training cost, expressed in node seconds. Populated for models trained in node time.",
          "type": "string",
          "format": "int64"
        },
        "exportModelSpec": {
          "description": "Information of downloadable models that are pre-generated as part of training flow and will be persisted in AutoMl backend. Populated for AutoMl requests.",
          "$ref": "XPSImageExportModelSpec"
        }
      },
      "type": "object",
      "id": "XPSImageClassificationTrainResponse"
    },
    "XPSEvaluationMetrics": {
      "id": "XPSEvaluationMetrics",
      "properties": {
        "translationEvalMetrics": {
          "$ref": "XPSTranslationEvaluationMetrics"
        },
        "videoActionRecognitionEvalMetrics": {
          "$ref": "XPSVideoActionRecognitionEvaluationMetrics"
        },
        "imageSegmentationEvalMetrics": {
          "$ref": "XPSImageSegmentationEvaluationMetrics"
        },
        "annotationSpecIdToken": {
          "description": "The annotation_spec for which this evaluation metrics instance had been created. Empty iff this is an overall model evaluation (like Tables evaluation metrics), i.e. aggregated across all labels. The value comes from the input annotations in AnnotatedExample. For MVP product or for text sentiment models where annotation_spec_id_token is not available, set label instead.",
          "type": "string"
        },
        "evaluatedExampleCount": {
          "description": "The number of examples used to create this evaluation metrics instance.",
          "type": "integer",
          "format": "int32"
        },
        "category": {
          "description": "The integer category label for which this evaluation metric instance had been created. Valid categories are 0 or higher. Overall model evaluation should set this to negative values (rather than implicit zero). Only used for Image Segmentation (prefer to set annotation_spec_id_token instead). Note: uCAIP Image Segmentation should use annotation_spec_id_token.",
          "format": "int32",
          "type": "integer"
        },
        "videoObjectTrackingEvalMetrics": {
          "$ref": "XPSVideoObjectTrackingEvaluationMetrics"
        },
        "imageClassificationEvalMetrics": {
          "$ref": "XPSClassificationEvaluationMetrics"
        },
        "textSentimentEvalMetrics": {
          "$ref": "XPSTextSentimentEvaluationMetrics"
        },
        "regressionEvalMetrics": {
          "$ref": "XPSRegressionEvaluationMetrics"
        },
        "textExtractionEvalMetrics": {
          "$ref": "XPSTextExtractionEvaluationMetrics"
        },
        "textClassificationEvalMetrics": {
          "$ref": "XPSClassificationEvaluationMetrics"
        },
        "tablesEvalMetrics": {
          "$ref": "XPSTablesEvaluationMetrics"
        },
        "imageObjectDetectionEvalMetrics": {
          "$ref": "XPSImageObjectDetectionEvaluationMetrics"
        },
        "label": {
          "description": "The label for which this evaluation metrics instance had been created. Empty iff this is an overall model evaluation (like Tables evaluation metrics), i.e. aggregated across all labels. The label maps to AnnotationSpec.display_name in Public API protos. Only used by MVP implementation and text sentiment FULL implementation.",
          "type": "string"
        },
        "tablesClassificationEvalMetrics": {
          "$ref": "XPSClassificationEvaluationMetrics"
        },
        "videoClassificationEvalMetrics": {
          "$ref": "XPSClassificationEvaluationMetrics"
        }
      },
      "description": "Contains xPS-specific model evaluation metrics either for a single annotation spec (label), or for the model overall. Next tag: 18.",
      "type": "object"
    },
    "PartOfSpeech": {
      "id": "PartOfSpeech",
      "type": "object",
      "properties": {
        "case": {
          "enumDescriptions": [
            "Case is not applicable in the analyzed language or is not predicted.",
            "Accusative",
            "Adverbial",
            "Complementive",
            "Dative",
            "Genitive",
            "Instrumental",
            "Locative",
            "Nominative",
            "Oblique",
            "Partitive",
            "Prepositional",
            "Reflexive",
            "Relative",
            "Vocative"
          ],
          "type": "string",
          "description": "The grammatical case.",
          "enum": [
            "CASE_UNKNOWN",
            "ACCUSATIVE",
            "ADVERBIAL",
            "COMPLEMENTIVE",
            "DATIVE",
            "GENITIVE",
            "INSTRUMENTAL",
            "LOCATIVE",
            "NOMINATIVE",
            "OBLIQUE",
            "PARTITIVE",
            "PREPOSITIONAL",
            "REFLEXIVE_CASE",
            "RELATIVE_CASE",
            "VOCATIVE"
          ]
        },
        "aspect": {
          "enum": [
            "ASPECT_UNKNOWN",
            "PERFECTIVE",
            "IMPERFECTIVE",
            "PROGRESSIVE"
          ],
          "description": "The grammatical aspect.",
          "enumDescriptions": [
            "Aspect is not applicable in the analyzed language or is not predicted.",
            "Perfective",
            "Imperfective",
            "Progressive"
          ],
          "type": "string"
        },
        "person": {
          "type": "string",
          "description": "The grammatical person.",
          "enum": [
            "PERSON_UNKNOWN",
            "FIRST",
            "SECOND",
            "THIRD",
            "REFLEXIVE_PERSON"
          ],
          "enumDescriptions": [
            "Person is not applicable in the analyzed language or is not predicted.",
            "First",
            "Second",
            "Third",
            "Reflexive"
          ]
        },
        "tense": {
          "description": "The grammatical tense.",
          "enumDescriptions": [
            "Tense is not applicable in the analyzed language or is not predicted.",
            "Conditional",
            "Future",
            "Past",
            "Present",
            "Imperfect",
            "Pluperfect"
          ],
          "enum": [
            "TENSE_UNKNOWN",
            "CONDITIONAL_TENSE",
            "FUTURE",
            "PAST",
            "PRESENT",
            "IMPERFECT",
            "PLUPERFECT"
          ],
          "type": "string"
        },
        "number": {
          "enum": [
            "NUMBER_UNKNOWN",
            "SINGULAR",
            "PLURAL",
            "DUAL"
          ],
          "enumDescriptions": [
            "Number is not applicable in the analyzed language or is not predicted.",
            "Singular",
            "Plural",
            "Dual"
          ],
          "description": "The grammatical number.",
          "type": "string"
        },
        "reciprocity": {
          "enum": [
            "RECIPROCITY_UNKNOWN",
            "RECIPROCAL",
            "NON_RECIPROCAL"
          ],
          "enumDescriptions": [
            "Reciprocity is not applicable in the analyzed language or is not predicted.",
            "Reciprocal",
            "Non-reciprocal"
          ],
          "type": "string",
          "description": "The grammatical reciprocity."
        },
        "mood": {
          "enum": [
            "MOOD_UNKNOWN",
            "CONDITIONAL_MOOD",
            "IMPERATIVE",
            "INDICATIVE",
            "INTERROGATIVE",
            "JUSSIVE",
            "SUBJUNCTIVE"
          ],
          "description": "The grammatical mood.",
          "enumDescriptions": [
            "Mood is not applicable in the analyzed language or is not predicted.",
            "Conditional",
            "Imperative",
            "Indicative",
            "Interrogative",
            "Jussive",
            "Subjunctive"
          ],
          "type": "string"
        },
        "gender": {
          "enumDescriptions": [
            "Gender is not applicable in the analyzed language or is not predicted.",
            "Feminine",
            "Masculine",
            "Neuter"
          ],
          "type": "string",
          "description": "The grammatical gender.",
          "enum": [
            "GENDER_UNKNOWN",
            "FEMININE",
            "MASCULINE",
            "NEUTER"
          ]
        },
        "form": {
          "type": "string",
          "enumDescriptions": [
            "Form is not applicable in the analyzed language or is not predicted.",
            "Adnomial",
            "Auxiliary",
            "Complementizer",
            "Final ending",
            "Gerund",
            "Realis",
            "Irrealis",
            "Short form",
            "Long form",
            "Order form",
            "Specific form"
          ],
          "enum": [
            "FORM_UNKNOWN",
            "ADNOMIAL",
            "AUXILIARY",
            "COMPLEMENTIZER",
            "FINAL_ENDING",
            "GERUND",
            "REALIS",
            "IRREALIS",
            "SHORT",
            "LONG",
            "ORDER",
            "SPECIFIC"
          ],
          "description": "The grammatical form."
        },
        "voice": {
          "type": "string",
          "enum": [
            "VOICE_UNKNOWN",
            "ACTIVE",
            "CAUSATIVE",
            "PASSIVE"
          ],
          "description": "The grammatical voice.",
          "enumDescriptions": [
            "Voice is not applicable in the analyzed language or is not predicted.",
            "Active",
            "Causative",
            "Passive"
          ]
        },
        "proper": {
          "type": "string",
          "description": "The grammatical properness.",
          "enum": [
            "PROPER_UNKNOWN",
            "PROPER",
            "NOT_PROPER"
          ],
          "enumDescriptions": [
            "Proper is not applicable in the analyzed language or is not predicted.",
            "Proper",
            "Not proper"
          ]
        },
        "tag": {
          "enum": [
            "UNKNOWN",
            "ADJ",
            "ADP",
            "ADV",
            "CONJ",
            "DET",
            "NOUN",
            "NUM",
            "PRON",
            "PRT",
            "PUNCT",
            "VERB",
            "X",
            "AFFIX"
          ],
          "description": "The part of speech tag.",
          "enumDescriptions": [
            "Unknown",
            "Adjective",
            "Adposition (preposition and postposition)",
            "Adverb",
            "Conjunction",
            "Determiner",
            "Noun (common and proper)",
            "Cardinal number",
            "Pronoun",
            "Particle or other function word",
            "Punctuation",
            "Verb (all tenses and modes)",
            "Other: foreign words, typos, abbreviations",
            "Affix"
          ],
          "type": "string"
        }
      },
      "description": "Represents part of speech information for a token."
    },
    "XPSTableSpec": {
      "id": "XPSTableSpec",
      "type": "object",
      "properties": {
        "columnSpecs": {
          "additionalProperties": {
            "$ref": "XPSColumnSpec"
          },
          "description": "Mapping from column id to column spec.",
          "type": "object"
        },
        "rowCount": {
          "format": "int64",
          "type": "string",
          "description": "The number of rows in the table."
        },
        "importedDataSizeInBytes": {
          "format": "int64",
          "description": "The total size of imported data of the table.",
          "type": "string"
        },
        "timeColumnId": {
          "description": "The id of the time column.",
          "format": "int32",
          "type": "integer"
        },
        "validRowCount": {
          "format": "int64",
          "type": "string",
          "description": "The number of valid rows."
        }
      }
    },
    "XPSCategoryStatsSingleCategoryStats": {
      "type": "object",
      "description": "The statistics of a single CATEGORY value.",
      "id": "XPSCategoryStatsSingleCategoryStats",
      "properties": {
        "value": {
          "type": "string",
          "description": "The CATEGORY value."
        },
        "count": {
          "description": "The number of occurrences of this value in the series.",
          "type": "string",
          "format": "int64"
        }
      }
    },
    "XPSImageObjectDetectionEvaluationMetrics": {
      "description": "Model evaluation metrics for image object detection problems. Evaluates prediction quality of labeled bounding boxes.",
      "properties": {
        "evaluatedBoundingBoxCount": {
          "type": "integer",
          "description": "The total number of bounding boxes (i.e. summed over all images) the ground truth used to create this evaluation had.",
          "format": "int32"
        },
        "boundingBoxMetricsEntries": {
          "type": "array",
          "description": "The bounding boxes match metrics for each Intersection-over-union threshold 0.05,0.10,...,0.95,0.96,0.97,0.98,0.99 and each label confidence threshold 0.05,0.10,...,0.95,0.96,0.97,0.98,0.99 pair.",
          "items": {
            "$ref": "XPSBoundingBoxMetricsEntry"
          }
        },
        "boundingBoxMeanAveragePrecision": {
          "description": "The single metric for bounding boxes evaluation: the mean_average_precision averaged over all bounding_box_metrics_entries.",
          "format": "float",
          "type": "number"
        }
      },
      "id": "XPSImageObjectDetectionEvaluationMetrics",
      "type": "object"
    },
    "XPSVisionErrorAnalysisConfig": {
      "id": "XPSVisionErrorAnalysisConfig",
      "type": "object",
      "description": "The vision model error analysis configuration. Next tag: 3",
      "properties": {
        "queryType": {
          "description": "The query type used in retrieval. The enum values are frozen in the foreseeable future.",
          "type": "string",
          "enumDescriptions": [
            "Unspecified query type for model error analysis.",
            "Query similar samples across all classes in the dataset.",
            "Query similar samples from the same class of the input sample.",
            "Query dissimilar samples from the same class of the input sample."
          ],
          "enum": [
            "QUERY_TYPE_UNSPECIFIED",
            "QUERY_TYPE_ALL_SIMILAR",
            "QUERY_TYPE_SAME_CLASS_SIMILAR",
            "QUERY_TYPE_SAME_CLASS_DISSIMILAR"
          ]
        },
        "exampleCount": {
          "description": "The number of query examples in error analysis.",
          "type": "integer",
          "format": "int32"
        }
      }
    },
    "AnnotateTextRequestFeatures": {
      "type": "object",
      "description": "All available features for sentiment, syntax, and semantic analysis. Setting each one to true will enable that specific analysis for the input. Next ID: 12",
      "id": "AnnotateTextRequestFeatures",
      "properties": {
        "extractSyntax": {
          "type": "boolean",
          "description": "Extract syntax information."
        },
        "extractEntities": {
          "type": "boolean",
          "description": "Extract entities."
        },
        "moderateText": {
          "description": "Moderate the document for harmful and sensitive categories.",
          "type": "boolean"
        },
        "classificationModelOptions": {
          "description": "Optional. The model options to use for classification. Defaults to v1 options if not specified. Only used if `classify_text` is set to true.",
          "$ref": "ClassificationModelOptions"
        },
        "classifyText": {
          "type": "boolean",
          "description": "Classify the full document into categories. If this is true, the API will use the default model which classifies into a [predefined taxonomy](https://cloud.google.com/natural-language/docs/categories)."
        },
        "extractEntitySentiment": {
          "description": "Extract entities and their associated sentiment.",
          "type": "boolean"
        },
        "extractDocumentSentiment": {
          "description": "Extract document-level sentiment.",
          "type": "boolean"
        }
      }
    },
    "XPSTablesConfidenceMetricsEntry": {
      "type": "object",
      "description": "Metrics for a single confidence threshold.",
      "id": "XPSTablesConfidenceMetricsEntry",
      "properties": {
        "truePositiveRate": {
          "description": "TPR = #true positives / (#true positives + #false negatvies)",
          "type": "number",
          "format": "double"
        },
        "confidenceThreshold": {
          "type": "number",
          "description": "The confidence threshold value used to compute the metrics.",
          "format": "double"
        },
        "precision": {
          "format": "double",
          "type": "number",
          "description": "Precision = #true positives / (#true positives + #false positives)."
        },
        "falsePositiveCount": {
          "type": "string",
          "description": "False positive count.",
          "format": "int64"
        },
        "falseNegativeCount": {
          "description": "False negative count.",
          "type": "string",
          "format": "int64"
        },
        "truePositiveCount": {
          "type": "string",
          "format": "int64",
          "description": "True positive count."
        },
        "recall": {
          "format": "double",
          "type": "number",
          "description": "Recall = #true positives / (#true positives + #false negatives)."
        },
        "falsePositiveRate": {
          "format": "double",
          "description": "FPR = #false positives / (#false positives + #true negatives)",
          "type": "number"
        },
        "trueNegativeCount": {
          "type": "string",
          "description": "True negative count.",
          "format": "int64"
        },
        "f1Score": {
          "description": "The harmonic mean of recall and precision. (2 * precision * recall) / (precision + recall)",
          "format": "double",
          "type": "number"
        }
      }
    },
    "XPSTuningTrial": {
      "id": "XPSTuningTrial",
      "type": "object",
      "properties": {
        "modelStructure": {
          "$ref": "XPSTablesModelStructure",
          "description": "Model parameters for the trial."
        },
        "trainingObjectivePoint": {
          "description": "The optimization objective evaluation of the eval split data.",
          "$ref": "XPSTrainingObjectivePoint"
        }
      },
      "description": "Metrics for a tuning job generated, will get forwarded to Stackdriver as model tuning logs. Setting this as a standalone message out of CreateModelMetadata to avoid confusion as we expose this message only to users."
    },
    "XPSPreprocessResponse": {
      "type": "object",
      "properties": {
        "speechPreprocessResp": {
          "$ref": "XPSSpeechPreprocessResponse"
        },
        "translationPreprocessResp": {
          "$ref": "XPSTranslationPreprocessResponse"
        },
        "outputExampleSet": {
          "description": "Preprocessed examples, that are to be imported into AutoML storage. This should point to RecordIO file(s) of PreprocessedExample messages. The PreprocessedExample.mvp_training_data-s returned here are later verbatim passed to Train() call in TrainExample.mvp_training_data.",
          "$ref": "XPSExampleSet"
        },
        "tablesPreprocessResponse": {
          "$ref": "XPSTablesPreprocessResponse"
        }
      },
      "description": "Next ID: 8",
      "id": "XPSPreprocessResponse"
    },
    "XPSImageModelServingSpecModelThroughputEstimation": {
      "properties": {
        "computeEngineAcceleratorType": {
          "enum": [
            "UNSPECIFIED",
            "NVIDIA_TESLA_K80",
            "NVIDIA_TESLA_P100",
            "NVIDIA_TESLA_V100",
            "NVIDIA_TESLA_P4",
            "NVIDIA_TESLA_T4",
            "NVIDIA_TESLA_A100",
            "NVIDIA_A100_80GB",
            "NVIDIA_L4",
            "NVIDIA_H100_80GB",
            "TPU_V2",
            "TPU_V3",
            "TPU_V4_POD",
            "TPU_V5_LITEPOD"
          ],
          "enumDescriptions": [
            "",
            "Nvidia Tesla K80 GPU.",
            "Nvidia Tesla P100 GPU.",
            "Nvidia Tesla V100 GPU.",
            "Nvidia Tesla P4 GPU.",
            "Nvidia Tesla T4 GPU.",
            "Nvidia Tesla A100 GPU.",
            "Nvidia A100 80GB GPU.",
            "Nvidia L4 GPU.",
            "Nvidia H100 80Gb GPU.",
            "TPU v2 (JellyFish).",
            "TPU v3 (DragonFish).",
            "TPU_v4 (PufferFish).",
            "TPU v5 Lite Pods."
          ],
          "type": "string"
        },
        "servomaticPartitionType": {
          "type": "string",
          "enum": [
            "PARTITION_TYPE_UNSPECIFIED",
            "PARTITION_ZERO",
            "PARTITION_REDUCED_HOMING",
            "PARTITION_JELLYFISH",
            "PARTITION_CPU",
            "PARTITION_CUSTOM_STORAGE_CPU"
          ],
          "enumDescriptions": [
            "",
            "The default partition.",
            "It has significantly lower replication than partition-0 and is located in the US only. It also has a larger model size limit and higher default RAM quota than partition-0. Customers with batch traffic, US-based traffic, or very large models should use this partition. Capacity in this partition is significantly cheaper than partition-0.",
            "To be used by customers with Jellyfish-accelerated ops. See go/servomatic-jellyfish for details.",
            "The partition used by regionalized servomatic cloud regions.",
            "The partition used for loading models from custom storage."
          ]
        },
        "nodeQps": {
          "format": "double",
          "type": "number",
          "description": "The approximate qps a deployed node can serve."
        },
        "latencyInMilliseconds": {
          "description": "Estimated latency.",
          "format": "double",
          "type": "number"
        }
      },
      "id": "XPSImageModelServingSpecModelThroughputEstimation",
      "type": "object"
    },
    "TextSpan": {
      "id": "TextSpan",
      "type": "object",
      "description": "Represents a text span in the input document.",
      "properties": {
        "beginOffset": {
          "description": "The API calculates the beginning offset of the content in the original document according to the EncodingType specified in the API request.",
          "type": "integer",
          "format": "int32"
        },
        "content": {
          "type": "string",
          "description": "The content of the text span, which is a substring of the document."
        }
      }
    },
    "XPSVideoActionMetricsEntry": {
      "type": "object",
      "description": "The Evaluation metrics entry given a specific precision_window_length.",
      "properties": {
        "meanAveragePrecision": {
          "description": "The mean average precision.",
          "type": "number",
          "format": "float"
        },
        "precisionWindowLength": {
          "format": "google-duration",
          "description": "This VideoActionMetricsEntry is calculated based on this prediction window length. If the predicted action's timestamp is inside the time window whose center is the ground truth action's timestamp with this specific length, the prediction result is treated as a true positive.",
          "type": "string"
        },
        "confidenceMetricsEntries": {
          "items": {
            "$ref": "XPSVideoActionMetricsEntryConfidenceMetricsEntry"
          },
          "type": "array",
          "description": "Metrics for each label-match confidence_threshold from 0.05,0.10,...,0.95,0.96,0.97,0.98,0.99."
        }
      },
      "id": "XPSVideoActionMetricsEntry"
    },
    "XPSMetricEntryLabel": {
      "properties": {
        "labelName": {
          "description": "The name of the label.",
          "type": "string"
        },
        "labelValue": {
          "description": "The value of the label.",
          "type": "string"
        }
      },
      "id": "XPSMetricEntryLabel",
      "type": "object"
    },
    "XPSFileSpec": {
      "type": "object",
      "properties": {
        "directoryPath": {
          "description": "Deprecated. Use file_spec.",
          "deprecated": true,
          "type": "string"
        },
        "fileFormat": {
          "type": "string",
          "enum": [
            "FILE_FORMAT_UNKNOWN",
            "FILE_FORMAT_SSTABLE",
            "FILE_FORMAT_TRANSLATION_RKV",
            "FILE_FORMAT_RECORDIO",
            "FILE_FORMAT_RAW_CSV",
            "FILE_FORMAT_RAW_CAPACITOR"
          ],
          "enumDescriptions": [
            "",
            "",
            "Internal format for parallel text data used by Google Translate. go/rkvtools",
            "",
            "Only the lexicographically first file described by the file_spec contains the header line.",
            ""
          ],
          "enumDeprecated": [
            false,
            true,
            false,
            false,
            false,
            false
          ]
        },
        "fileSpec": {
          "type": "string",
          "description": "Single file path, or file pattern of format \"/path/to/file@shard_count\". E.g. /cns/cell-d/somewhere/file@2 is expanded to two files: /cns/cell-d/somewhere/file-00000-of-00002 and /cns/cell-d/somewhere/file-00001-of-00002."
        },
        "singleFilePath": {
          "deprecated": true,
          "description": "Deprecated. Use file_spec.",
          "type": "string"
        }
      },
      "description": "Spec of input and output files, on external file systems (CNS, GCS, etc).",
      "id": "XPSFileSpec"
    },
    "XPSRegressionMetricsEntry": {
      "id": "XPSRegressionMetricsEntry",
      "description": "A pair of actual & observed values for the model being evaluated.",
      "properties": {
        "trueValue": {
          "format": "float",
          "description": "The actual target value for a row in the dataset.",
          "type": "number"
        },
        "predictedValue": {
          "description": "The observed value for a row in the dataset.",
          "type": "number",
          "format": "float"
        }
      },
      "type": "object"
    },
    "XPSVideoObjectTrackingEvaluationMetrics": {
      "description": "Model evaluation metrics for ObjectTracking problems. Next tag: 10.",
      "id": "XPSVideoObjectTrackingEvaluationMetrics",
      "properties": {
        "evaluatedTrackCount": {
          "format": "int32",
          "type": "integer",
          "description": "The number of tracks used for model evaluation."
        },
        "trackMeanMismatchRate": {
          "type": "number",
          "description": "Output only. The single metric for tracking consistency evaluation: the mean_mismatch_rate averaged over all track_metrics_entries.",
          "format": "float"
        },
        "evaluatedBoundingboxCount": {
          "type": "integer",
          "format": "int32",
          "description": "The number of bounding boxes used for model evaluation."
        },
        "evaluatedFrameCount": {
          "type": "integer",
          "format": "int32",
          "description": "The number of video frames used for model evaluation."
        },
        "boundingBoxMeanAveragePrecision": {
          "format": "float",
          "description": "Output only. The single metric for bounding boxes evaluation: the mean_average_precision averaged over all bounding_box_metrics_entries.",
          "type": "number"
        },
        "trackMeanBoundingBoxIou": {
          "type": "number",
          "description": "Output only. The single metric for tracks bounding box iou evaluation: the mean_bounding_box_iou averaged over all track_metrics_entries.",
          "format": "float"
        },
        "trackMetricsEntries": {
          "items": {
            "$ref": "XPSTrackMetricsEntry"
          },
          "description": "Output only. The tracks match metrics for each Intersection-over-union threshold 0.05,0.10,...,0.95,0.96,0.97,0.98,0.99.",
          "type": "array"
        },
        "boundingBoxMetricsEntries": {
          "description": "Output only. The bounding boxes match metrics for each Intersection-over-union threshold 0.05,0.10,...,0.95,0.96,0.97,0.98,0.99.",
          "items": {
            "$ref": "XPSBoundingBoxMetricsEntry"
          },
          "type": "array"
        },
        "trackMeanAveragePrecision": {
          "type": "number",
          "format": "float",
          "description": "Output only. The single metric for tracks accuracy evaluation: the mean_average_precision averaged over all track_metrics_entries."
        }
      },
      "type": "object"
    },
    "XPSTranslationTrainResponse": {
      "description": "Train response for translation.",
      "properties": {
        "modelType": {
          "enum": [
            "MODEL_TYPE_UNSPECIFIED",
            "LEGACY",
            "CURRENT"
          ],
          "enumDescriptions": [
            "Default",
            "Legacy model. Will be deprecated.",
            "Current model."
          ],
          "description": "Type of the model.",
          "type": "string"
        }
      },
      "type": "object",
      "id": "XPSTranslationTrainResponse"
    },
    "XPSTrackMetricsEntry": {
      "id": "XPSTrackMetricsEntry",
      "properties": {
        "iouThreshold": {
          "format": "float",
          "description": "Output only. The intersection-over-union threshold value between bounding boxes across frames used to compute this metric entry.",
          "type": "number"
        },
        "confidenceMetricsEntries": {
          "items": {
            "$ref": "XPSTrackMetricsEntryConfidenceMetricsEntry"
          },
          "description": "Output only. Metrics for each label-match confidence_threshold from 0.05,0.10,...,0.95,0.96,0.97,0.98,0.99. Precision-recall curve is derived from them.",
          "type": "array"
        },
        "meanTrackingAveragePrecision": {
          "format": "float",
          "type": "number",
          "description": "Output only. The mean average precision over all confidence thresholds."
        },
        "meanMismatchRate": {
          "format": "float",
          "type": "number",
          "description": "Output only. The mean mismatch rate over all confidence thresholds."
        },
        "meanBoundingBoxIou": {
          "type": "number",
          "description": "Output only. The mean bounding box iou over all confidence thresholds.",
          "format": "float"
        }
      },
      "type": "object",
      "description": "Track matching model metrics for a single track match threshold and multiple label match confidence thresholds. Next tag: 6."
    },
    "XPSVisionTrainingOperationMetadata": {
      "type": "object",
      "deprecated": true,
      "properties": {
        "explanationUsage": {
          "$ref": "InfraUsage",
          "description": "Aggregated infra usage within certain time period, for billing report purpose if XAI is enable in training request."
        }
      },
      "id": "XPSVisionTrainingOperationMetadata"
    },
    "XPSTrainResponse": {
      "properties": {
        "textTrainResp": {
          "description": "Will only be needed for uCAIP from Beta.",
          "$ref": "XPSTextTrainResponse"
        },
        "evaluationMetricsSet": {
          "description": "The trained model evaluation metrics. This can be optionally returned.",
          "$ref": "XPSEvaluationMetricsSet"
        },
        "modelToken": {
          "description": "Token that represents the trained model. This is considered immutable and is persisted in AutoML. xPS can put their own proto in the byte string, to e.g. point to the model checkpoints. The token is passed to other xPS APIs to refer to the model.",
          "format": "byte",
          "type": "string"
        },
        "imageClassificationTrainResp": {
          "$ref": "XPSImageClassificationTrainResponse"
        },
        "deployedModelSizeBytes": {
          "type": "string",
          "format": "int64",
          "description": "Estimated model size in bytes once deployed."
        },
        "textToSpeechTrainResp": {
          "$ref": "XPSTextToSpeechTrainResponse"
        },
        "speechTrainResp": {
          "$ref": "XPSSpeechModelSpec"
        },
        "translationTrainResp": {
          "$ref": "XPSTranslationTrainResponse"
        },
        "videoObjectTrackingTrainResp": {
          "$ref": "XPSVideoObjectTrackingTrainResponse"
        },
        "imageObjectDetectionTrainResp": {
          "$ref": "XPSImageObjectDetectionModelSpec"
        },
        "evaluatedExampleSet": {
          "$ref": "XPSExampleSet",
          "description": "Examples used to evaluate the model (usually the test set), with the predicted annotations. The file_spec should point to recordio file(s) of AnnotatedExample. For each returned example, the example_id_token and annotations predicted by the model must be set. The example payload can and is recommended to be omitted."
        },
        "videoActionRecognitionTrainResp": {
          "$ref": "XPSVideoActionRecognitionTrainResponse"
        },
        "tablesTrainResp": {
          "$ref": "XPSTablesTrainResponse"
        },
        "explanationConfigs": {
          "description": "VisionExplanationConfig for XAI on test set. Optional for when XAI is enable in training request.",
          "deprecated": true,
          "items": {
            "$ref": "XPSResponseExplanationSpec"
          },
          "type": "array"
        },
        "errorAnalysisConfigs": {
          "description": "Optional vision model error analysis configuration. The field is set when model error analysis is enabled in the training request. The results of error analysis will be binded together with evaluation results (in the format of AnnotatedExample).",
          "type": "array",
          "items": {
            "$ref": "XPSVisionErrorAnalysisConfig"
          }
        },
        "videoClassificationTrainResp": {
          "$ref": "XPSVideoClassificationTrainResponse"
        },
        "imageSegmentationTrainResp": {
          "$ref": "XPSImageSegmentationTrainResponse"
        }
      },
      "id": "XPSTrainResponse",
      "description": "Next ID: 18",
      "type": "object"
    },
    "XPSDataType": {
      "description": "Indicated the type of data that can be stored in a structured data entity (e.g. a table).",
      "type": "object",
      "properties": {
        "typeCode": {
          "description": "Required. The TypeCode for this type.",
          "type": "string",
          "enumDescriptions": [
            "Not specified. Should not be used.",
            "Encoded as `number`, or the strings `\"NaN\"`, `\"Infinity\"`, or `\"-Infinity\"`.",
            "Must be between 0AD and 9999AD. Encoded as `string` according to time_format, or, if that format is not set, then in RFC 3339 `date-time` format, where `time-offset` = `\"Z\"` (e.g. 1985-04-12T23:20:50.52Z).",
            "Encoded as `string`.",
            "Encoded as `list`, where the list elements are represented according to list_element_type.",
            "Encoded as `struct`, where field values are represented according to struct_type.",
            "Values of this type are not further understood by AutoML, e.g. AutoML is unable to tell the order of values (as it could with FLOAT64), or is unable to say if one value contains another (as it could with STRING). Encoded as `string` (bytes should be base64-encoded, as described in RFC 4648, section 4)."
          ],
          "enum": [
            "TYPE_CODE_UNSPECIFIED",
            "FLOAT64",
            "TIMESTAMP",
            "STRING",
            "ARRAY",
            "STRUCT",
            "CATEGORY"
          ]
        },
        "timeFormat": {
          "type": "string",
          "description": "If type_code == TIMESTAMP then `time_format` provides the format in which that time field is expressed. The time_format must be written in `strftime` syntax. If time_format is not set, then the default format as described on the field is used."
        },
        "compatibleDataTypes": {
          "description": "The highly compatible data types to this data type.",
          "items": {
            "$ref": "XPSDataType"
          },
          "type": "array"
        },
        "listElementType": {
          "description": "If type_code == ARRAY, then `list_element_type` is the type of the elements.",
          "$ref": "XPSDataType"
        },
        "nullable": {
          "description": "If true, this DataType can also be `null`.",
          "type": "boolean"
        },
        "structType": {
          "$ref": "XPSStructType",
          "description": "If type_code == STRUCT, then `struct_type` provides type information for the struct's fields."
        }
      },
      "id": "XPSDataType"
    },
    "XPSExampleSet": {
      "id": "XPSExampleSet",
      "properties": {
        "fingerprint": {
          "type": "string",
          "description": "Fingerprint of the example set.",
          "format": "int64"
        },
        "numInputSources": {
          "format": "int64",
          "description": "Number of input sources.",
          "type": "string"
        },
        "numExamples": {
          "format": "int64",
          "description": "Number of examples.",
          "type": "string"
        },
        "fileSpec": {
          "description": "File spec of the examples or input sources.",
          "$ref": "XPSFileSpec"
        }
      },
      "type": "object",
      "description": "Set of examples or input sources."
    },
    "XPSTfJsFormat": {
      "description": "A [TensorFlow.js](https://www.tensorflow.org/js) model that can be used in the browser and in Node.js using JavaScript.",
      "type": "object",
      "id": "XPSTfJsFormat",
      "properties": {}
    },
    "XPSTablesModelStructureModelParametersParameter": {
      "id": "XPSTablesModelStructureModelParametersParameter",
      "properties": {
        "stringValue": {
          "type": "string",
          "description": "String type parameter value."
        },
        "intValue": {
          "format": "int64",
          "type": "string",
          "description": "Integer type parameter value."
        },
        "floatValue": {
          "description": "Float type parameter value.",
          "format": "double",
          "type": "number"
        },
        "name": {
          "type": "string",
          "description": "Parameter name."
        }
      },
      "type": "object"
    },
    "XPSTablesDatasetMetadata": {
      "id": "XPSTablesDatasetMetadata",
      "properties": {
        "primaryTableSpec": {
          "$ref": "XPSTableSpec",
          "description": "Primary table."
        },
        "mlUseColumnId": {
          "format": "int32",
          "type": "integer",
          "description": "Id the column to split the table."
        },
        "targetColumnId": {
          "format": "int32",
          "description": "Id of the primary table column that should be used as the training label.",
          "type": "integer"
        },
        "targetColumnCorrelations": {
          "additionalProperties": {
            "$ref": "XPSCorrelationStats"
          },
          "type": "object",
          "description": "(the column id : its CorrelationStats with target column)."
        },
        "weightColumnId": {
          "format": "int32",
          "type": "integer",
          "description": "Id of the primary table column that should be used as the weight column."
        }
      },
      "description": "Metadata for a dataset used for AutoML Tables. Next ID: 6",
      "type": "object"
    },
    "XPSVideoActionRecognitionEvaluationMetrics": {
      "id": "XPSVideoActionRecognitionEvaluationMetrics",
      "description": "Model evaluation metrics for video action recognition.",
      "properties": {
        "evaluatedActionCount": {
          "format": "int32",
          "type": "integer",
          "description": "Output only. The number of ground truth actions used to create this evaluation."
        },
        "videoActionMetricsEntries": {
          "type": "array",
          "items": {
            "$ref": "XPSVideoActionMetricsEntry"
          },
          "description": "Output only. The metric entries for precision window lengths: 1s,2s,3s,4s, 5s."
        }
      },
      "type": "object"
    },
    "EntityMention": {
      "description": "Represents a mention for an entity in the text. Currently, proper noun mentions are supported.",
      "id": "EntityMention",
      "properties": {
        "type": {
          "enum": [
            "TYPE_UNKNOWN",
            "PROPER",
            "COMMON"
          ],
          "enumDescriptions": [
            "Unknown",
            "Proper name",
            "Common noun (or noun compound)"
          ],
          "type": "string",
          "description": "The type of the entity mention."
        },
        "sentiment": {
          "$ref": "Sentiment",
          "description": "For calls to AnalyzeEntitySentiment or if AnnotateTextRequest.Features.extract_entity_sentiment is set to true, this field will contain the sentiment expressed for this mention of the entity in the provided document."
        },
        "text": {
          "$ref": "TextSpan",
          "description": "The mention text."
        }
      },
      "type": "object"
    },
    "XPSSpeechPreprocessResponse": {
      "id": "XPSSpeechPreprocessResponse",
      "properties": {
        "cnsTrainDataPath": {
          "description": "Location of shards of sstables (training data) of DataUtterance protos.",
          "type": "string"
        },
        "speechPreprocessStats": {
          "description": "Stats associated with the data.",
          "$ref": "XPSSpeechPreprocessStats"
        },
        "prebuiltModelEvaluationMetrics": {
          "$ref": "XPSSpeechEvaluationMetrics",
          "description": "The metrics for prebuilt speech models. They are included here because there is no prebuilt speech models stored in the AutoML."
        },
        "cnsTestDataPath": {
          "type": "string",
          "description": "Location od shards of sstables (test data) of DataUtterance protos."
        }
      },
      "type": "object"
    },
    "XPSTimestampStats": {
      "id": "XPSTimestampStats",
      "description": "The data statistics of a series of TIMESTAMP values.",
      "type": "object",
      "properties": {
        "medianTimestampNanos": {
          "type": "string",
          "format": "int64"
        },
        "commonStats": {
          "$ref": "XPSCommonStats"
        },
        "granularStats": {
          "type": "object",
          "description": "The string key is the pre-defined granularity. Currently supported: hour_of_day, day_of_week, month_of_year. Granularities finer that the granularity of timestamp data are not populated (e.g. if timestamps are at day granularity, then hour_of_day is not populated).",
          "additionalProperties": {
            "$ref": "XPSTimestampStatsGranularStats"
          }
        }
      }
    },
    "XPSTablesEvaluationMetrics": {
      "id": "XPSTablesEvaluationMetrics",
      "type": "object",
      "properties": {
        "classificationMetrics": {
          "description": "Classification metrics.",
          "$ref": "XPSTablesClassificationMetrics"
        },
        "regressionMetrics": {
          "$ref": "XPSTablesRegressionMetrics",
          "description": "Regression metrics."
        }
      }
    },
    "XPSXpsOperationMetadata": {
      "properties": {
        "videoBatchPredictOperationMetadata": {
          "$ref": "XPSVideoBatchPredictOperationMetadata"
        },
        "tablesTrainingOperationMetadata": {
          "$ref": "XPSTablesTrainingOperationMetadata"
        },
        "videoTrainingOperationMetadata": {
          "$ref": "XPSVideoTrainingOperationMetadata"
        },
        "exampleCount": {
          "type": "string",
          "format": "int64",
          "description": "Optional. XPS server can opt to provide example count of the long running operation (e.g. training, data importing, batch prediction)."
        },
        "reportingMetrics": {
          "$ref": "XPSReportingMetrics",
          "description": "Metrics for the operation. By the time the operation is terminated (whether succeeded or failed) as returned from XPS, AutoML BE assumes the metrics are finalized. AutoML BE transparently posts the metrics to Chemist if it's not empty, regardless of the response content or error type. If user is supposed to be charged in case of cancellation/error, this field should be set. In the case where the type of LRO doesn't require any billing, this field should be left unset."
        },
        "visionTrainingOperationMetadata": {
          "$ref": "XPSVisionTrainingOperationMetadata"
        }
      },
      "type": "object",
      "id": "XPSXpsOperationMetadata"
    },
    "AnalyzeSyntaxRequest": {
      "description": "The syntax analysis request message.",
      "properties": {
        "document": {
          "$ref": "Document",
          "description": "Required. Input document."
        },
        "encodingType": {
          "enumDescriptions": [
            "If `EncodingType` is not specified, encoding-dependent information (such as `begin_offset`) will be set at `-1`.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-8 encoding of the input. C++ and Go are examples of languages that use this encoding natively.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-16 encoding of the input. Java and JavaScript are examples of languages that use this encoding natively.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-32 encoding of the input. Python is an example of a language that uses this encoding natively."
          ],
          "type": "string",
          "enum": [
            "NONE",
            "UTF8",
            "UTF16",
            "UTF32"
          ],
          "description": "The encoding type used by the API to calculate offsets."
        }
      },
      "type": "object",
      "id": "AnalyzeSyntaxRequest"
    },
    "XPSStringStats": {
      "id": "XPSStringStats",
      "properties": {
        "commonStats": {
          "$ref": "XPSCommonStats"
        },
        "topUnigramStats": {
          "type": "array",
          "items": {
            "$ref": "XPSStringStatsUnigramStats"
          },
          "description": "The statistics of the top 20 unigrams, ordered by StringStats.UnigramStats.count."
        }
      },
      "description": "The data statistics of a series of STRING values.",
      "type": "object"
    },
    "XPSTextExtractionEvaluationMetrics": {
      "id": "XPSTextExtractionEvaluationMetrics",
      "properties": {
        "bestF1ConfidenceMetrics": {
          "$ref": "XPSConfidenceMetricsEntry",
          "description": "Values are at the highest F1 score on the precision-recall curve. Only confidence_threshold, recall, precision, and f1_score will be set.",
          "deprecated": true
        },
        "confusionMatrix": {
          "description": "Confusion matrix of the model, at the default confidence threshold (0.0). Only set for whole-model evaluation, not for evaluation per label.",
          "$ref": "XPSConfusionMatrix"
        },
        "perLabelConfidenceMetrics": {
          "description": "Only recall, precision, and f1_score will be set.",
          "deprecated": true,
          "additionalProperties": {
            "$ref": "XPSConfidenceMetricsEntry"
          },
          "type": "object"
        },
        "confidenceMetricsEntries": {
          "type": "array",
          "items": {
            "$ref": "XPSConfidenceMetricsEntry"
          },
          "description": "If the enclosing EvaluationMetrics.label is empty, confidence_metrics_entries is an evaluation of the entire model across all labels. If the enclosing EvaluationMetrics.label is set, confidence_metrics_entries applies to that label."
        }
      },
      "type": "object"
    },
    "Entity": {
      "properties": {
        "name": {
          "type": "string",
          "description": "The representative name for the entity."
        },
        "type": {
          "description": "The entity type.",
          "enum": [
            "UNKNOWN",
            "PERSON",
            "LOCATION",
            "ORGANIZATION",
            "EVENT",
            "WORK_OF_ART",
            "CONSUMER_GOOD",
            "OTHER",
            "PHONE_NUMBER",
            "ADDRESS",
            "DATE",
            "NUMBER",
            "PRICE"
          ],
          "enumDescriptions": [
            "Unknown",
            "Person",
            "Location",
            "Organization",
            "Event",
            "Artwork",
            "Consumer product",
            "Other types of entities",
            "Phone number The metadata lists the phone number, formatted according to local convention, plus whichever additional elements appear in the text: * `number` - the actual number, broken down into sections as per local convention * `national_prefix` - country code, if detected * `area_code` - region or area code, if detected * `extension` - phone extension (to be dialed after connection), if detected",
            "Address The metadata identifies the street number and locality plus whichever additional elements appear in the text: * `street_number` - street number * `locality` - city or town * `street_name` - street/route name, if detected * `postal_code` - postal code, if detected * `country` - country, if detected\u003c * `broad_region` - administrative area, such as the state, if detected * `narrow_region` - smaller administrative area, such as county, if detected * `sublocality` - used in Asian addresses to demark a district within a city, if detected",
            "Date The metadata identifies the components of the date: * `year` - four digit year, if detected * `month` - two digit month number, if detected * `day` - two digit day number, if detected",
            "Number The metadata is the number itself.",
            "Price The metadata identifies the `value` and `currency`."
          ],
          "type": "string"
        },
        "metadata": {
          "additionalProperties": {
            "type": "string"
          },
          "type": "object",
          "description": "Metadata associated with the entity. For most entity types, the metadata is a Wikipedia URL (`wikipedia_url`) and Knowledge Graph MID (`mid`), if they are available. For the metadata associated with other entity types, see the Type table below."
        },
        "salience": {
          "description": "The salience score associated with the entity in the [0, 1.0] range. The salience score for an entity provides information about the importance or centrality of that entity to the entire document text. Scores closer to 0 are less salient, while scores closer to 1.0 are highly salient.",
          "format": "float",
          "type": "number"
        },
        "mentions": {
          "items": {
            "$ref": "EntityMention"
          },
          "type": "array",
          "description": "The mentions of this entity in the input document. The API currently supports proper noun mentions."
        },
        "sentiment": {
          "$ref": "Sentiment",
          "description": "For calls to AnalyzeEntitySentiment or if AnnotateTextRequest.Features.extract_entity_sentiment is set to true, this field will contain the aggregate sentiment expressed for this entity in the provided document."
        }
      },
      "type": "object",
      "id": "Entity",
      "description": "Represents a phrase in the text that is a known entity, such as a person, an organization, or location. The API associates information, such as salience and mentions, with entities."
    },
    "XPSTablesTrainResponse": {
      "type": "object",
      "id": "XPSTablesTrainResponse",
      "properties": {
        "tablesModelColumnInfo": {
          "type": "array",
          "description": "Output only. Auxiliary information for each of the input_feature_column_specs, with respect to this particular model.",
          "items": {
            "$ref": "XPSTablesModelColumnInfo"
          }
        },
        "predictionSampleRows": {
          "items": {
            "$ref": "XPSRow"
          },
          "description": "Sample rows from the dataset this model was trained.",
          "type": "array"
        },
        "trainCostMilliNodeHours": {
          "type": "string",
          "format": "int64",
          "description": "The actual training cost of the model, expressed in milli node hours, i.e. 1,000 value in this field means 1 node hour. Guaranteed to not exceed the train budget."
        },
        "modelStructure": {
          "$ref": "XPSTablesModelStructure"
        }
      }
    },
    "XPSImageExportModelSpec": {
      "type": "object",
      "id": "XPSImageExportModelSpec",
      "description": "Information of downloadable models that are pre-generated as part of training flow and will be persisted in AutoMl backend. Upon receiving ExportModel request from user, AutoMl backend can serve the pre-generated models to user if exists (by copying the files from internal path to user provided location), otherwise, AutoMl backend will call xPS ExportModel API to generate the model on the fly with the requesting format.",
      "properties": {
        "exportModelOutputConfig": {
          "description": "Contains the model format and internal location of the model files to be exported/downloaded. Use the GCS bucket name which is provided via TrainRequest.gcs_bucket_name to store the model files.",
          "type": "array",
          "items": {
            "$ref": "XPSExportModelOutputConfig"
          }
        }
      }
    },
    "XPSTablesClassificationMetrics": {
      "properties": {
        "curveMetrics": {
          "type": "array",
          "items": {
            "$ref": "XPSTablesClassificationMetricsCurveMetrics"
          },
          "description": "Metrics building a curve."
        }
      },
      "description": "Metrics for Tables classification problems.",
      "id": "XPSTablesClassificationMetrics",
      "type": "object"
    },
    "XPSBoundingBoxMetricsEntry": {
      "properties": {
        "meanAveragePrecision": {
          "description": "The mean average precision.",
          "format": "float",
          "type": "number"
        },
        "iouThreshold": {
          "type": "number",
          "format": "float",
          "description": "The intersection-over-union threshold value used to compute this metrics entry."
        },
        "confidenceMetricsEntries": {
          "type": "array",
          "description": "Metrics for each label-match confidence_threshold from 0.05,0.10,...,0.95,0.96,0.97,0.98,0.99.",
          "items": {
            "$ref": "XPSBoundingBoxMetricsEntryConfidenceMetricsEntry"
          }
        }
      },
      "description": "Bounding box matching model metrics for a single intersection-over-union threshold and multiple label match confidence thresholds.",
      "type": "object",
      "id": "XPSBoundingBoxMetricsEntry"
    },
    "DiskMetric": {
      "type": "object",
      "properties": {
        "gibSec": {
          "description": "Required. Seconds of physical disk usage, e.g. 3600.",
          "format": "int64",
          "type": "string"
        },
        "diskType": {
          "description": "Required. Type of Disk, e.g. REGIONAL_SSD.",
          "type": "string",
          "enumDescriptions": [
            "",
            "",
            "",
            "",
            "",
            ""
          ],
          "enum": [
            "UNKNOWN_DISK_TYPE",
            "REGIONAL_SSD",
            "REGIONAL_STORAGE",
            "PD_SSD",
            "PD_STANDARD",
            "STORAGE_SNAPSHOT"
          ]
        }
      },
      "id": "DiskMetric"
    },
    "XPSStructType": {
      "type": "object",
      "id": "XPSStructType",
      "properties": {
        "fields": {
          "description": "Unordered map of struct field names to their data types.",
          "additionalProperties": {
            "$ref": "XPSDataType"
          },
          "type": "object"
        }
      },
      "description": "`StructType` defines the DataType-s of a STRUCT type."
    },
    "AnalyzeEntitySentimentResponse": {
      "properties": {
        "language": {
          "description": "The language of the text, which will be the same as the language specified in the request or, if not specified, the automatically-detected language. See Document.language field for more details.",
          "type": "string"
        },
        "entities": {
          "items": {
            "$ref": "Entity"
          },
          "description": "The recognized entities in the input document with associated sentiments.",
          "type": "array"
        }
      },
      "description": "The entity-level sentiment analysis response message.",
      "type": "object",
      "id": "AnalyzeEntitySentimentResponse"
    },
    "XPSSpeechEvaluationMetrics": {
      "properties": {
        "subModelEvaluationMetrics": {
          "description": "Evaluation metrics for all submodels contained in this model.",
          "type": "array",
          "items": {
            "$ref": "XPSSpeechEvaluationMetricsSubModelEvaluationMetric"
          }
        }
      },
      "id": "XPSSpeechEvaluationMetrics",
      "type": "object"
    },
    "XPSFloat64Stats": {
      "type": "object",
      "properties": {
        "quantiles": {
          "description": "Ordered from 0 to k k-quantile values of the data series of n values. The value at index i is, approximately, the i*n/k-th smallest value in the series; for i = 0 and i = k these are, respectively, the min and max values.",
          "type": "array",
          "items": {
            "type": "number",
            "format": "double"
          }
        },
        "commonStats": {
          "$ref": "XPSCommonStats"
        },
        "standardDeviation": {
          "description": "The standard deviation of the series.",
          "format": "double",
          "type": "number"
        },
        "mean": {
          "description": "The mean of the series.",
          "format": "double",
          "type": "number"
        },
        "histogramBuckets": {
          "type": "array",
          "items": {
            "$ref": "XPSFloat64StatsHistogramBucket"
          },
          "description": "Histogram buckets of the data series. Sorted by the min value of the bucket, ascendingly, and the number of the buckets is dynamically generated. The buckets are non-overlapping and completely cover whole FLOAT64 range with min of first bucket being `\"-Infinity\"`, and max of the last one being `\"Infinity\"`."
        }
      },
      "description": "The data statistics of a series of FLOAT64 values.",
      "id": "XPSFloat64Stats"
    },
    "XPSImageSegmentationTrainResponse": {
      "properties": {
        "trainCostNodeSeconds": {
          "type": "string",
          "description": "The actual train cost of creating this model, expressed in node seconds, i.e. 3,600 value in this field means 1 node hour.",
          "format": "int64"
        },
        "exportModelSpec": {
          "description": "NOTE: These fields are not used/needed in EAP but will be set later.",
          "$ref": "XPSImageExportModelSpec"
        },
        "modelServingSpec": {
          "$ref": "XPSImageModelServingSpec"
        },
        "colorMaps": {
          "type": "array",
          "items": {
            "$ref": "XPSColorMap"
          },
          "description": "Color map of the model."
        },
        "stopReason": {
          "description": "Stop reason for training job, e.g. 'TRAIN_BUDGET_REACHED', 'MODEL_CONVERGED'.",
          "enum": [
            "TRAIN_STOP_REASON_UNSPECIFIED",
            "TRAIN_STOP_REASON_BUDGET_REACHED",
            "TRAIN_STOP_REASON_MODEL_CONVERGED",
            "TRAIN_STOP_REASON_MODEL_EARLY_STOPPED"
          ],
          "enumDescriptions": [
            "",
            "",
            "Model fully converged, can not be resumbed training.",
            "Model early converged, can be further trained till full convergency."
          ],
          "type": "string"
        },
        "modelArtifactSpec": {
          "description": "## The fields below are only populated under uCAIP request scope. Model artifact spec stores and model gcs pathes and related metadata",
          "$ref": "XPSImageModelArtifactSpec"
        }
      },
      "type": "object",
      "id": "XPSImageSegmentationTrainResponse"
    },
    "XPSResponseExplanationMetadata": {
      "properties": {
        "outputs": {
          "description": "Metadata of the output.",
          "type": "object",
          "additionalProperties": {
            "$ref": "XPSResponseExplanationMetadataOutputMetadata"
          }
        },
        "inputs": {
          "type": "object",
          "description": "Metadata of the input.",
          "additionalProperties": {
            "$ref": "XPSResponseExplanationMetadataInputMetadata"
          }
        }
      },
      "deprecated": true,
      "type": "object",
      "id": "XPSResponseExplanationMetadata"
    },
    "XPSConfidenceMetricsEntry": {
      "description": "ConfidenceMetricsEntry includes generic precision, recall, f1 score etc. Next tag: 16.",
      "properties": {
        "f1ScoreAt1": {
          "format": "float",
          "type": "number",
          "description": "The harmonic mean of recall_at1 and precision_at1."
        },
        "recall": {
          "type": "number",
          "description": "Recall (true positive rate) for the given confidence threshold.",
          "format": "float"
        },
        "recallAt1": {
          "type": "number",
          "format": "float",
          "description": "The recall (true positive rate) when only considering the label that has the highest prediction score and not below the confidence threshold for each example."
        },
        "f1Score": {
          "type": "number",
          "description": "The harmonic mean of recall and precision.",
          "format": "float"
        },
        "confidenceThreshold": {
          "type": "number",
          "format": "float",
          "description": "Metrics are computed with an assumption that the model never return predictions with score lower than this value."
        },
        "truePositiveCount": {
          "type": "string",
          "format": "int64",
          "description": "The number of model created labels that match a ground truth label."
        },
        "falsePositiveRate": {
          "format": "float",
          "type": "number",
          "description": "False Positive Rate for the given confidence threshold."
        },
        "falsePositiveRateAt1": {
          "type": "number",
          "description": "The False Positive Rate when only considering the label that has the highest prediction score and not below the confidence threshold for each example.",
          "format": "float"
        },
        "falsePositiveCount": {
          "description": "The number of model created labels that do not match a ground truth label.",
          "type": "string",
          "format": "int64"
        },
        "precisionAt1": {
          "type": "number",
          "format": "float",
          "description": "The precision when only considering the label that has the highest prediction score and not below the confidence threshold for each example."
        },
        "precision": {
          "format": "float",
          "type": "number",
          "description": "Precision for the given confidence threshold."
        },
        "trueNegativeCount": {
          "type": "string",
          "description": "The number of labels that were not created by the model, but if they would, they would not match a ground truth label.",
          "format": "int64"
        },
        "falseNegativeCount": {
          "format": "int64",
          "description": "The number of ground truth labels that are not matched by a model created label.",
          "type": "string"
        },
        "positionThreshold": {
          "format": "int32",
          "type": "integer",
          "description": "Metrics are computed with an assumption that the model always returns at most this many predictions (ordered by their score, descendingly), but they all still need to meet the confidence_threshold."
        }
      },
      "type": "object",
      "id": "XPSConfidenceMetricsEntry"
    },
    "XPSResponseExplanationSpec": {
      "description": "Specification of Model explanation. Feature-based XAI in AutoML Vision ICN is deprecated, see b/288407203 for context.",
      "properties": {
        "explanationType": {
          "type": "string",
          "description": "Explanation type. For AutoML Image Classification models, possible values are: * `image-integrated-gradients` * `image-xrai`"
        },
        "parameters": {
          "description": "Parameters that configure explaining of the Model's predictions.",
          "$ref": "XPSResponseExplanationParameters"
        },
        "metadata": {
          "description": "Metadata describing the Model's input and output for explanation.",
          "$ref": "XPSResponseExplanationMetadata"
        }
      },
      "type": "object",
      "deprecated": true,
      "id": "XPSResponseExplanationSpec"
    },
    "XPSImageModelArtifactSpec": {
      "id": "XPSImageModelArtifactSpec",
      "properties": {
        "checkpointArtifact": {
          "$ref": "XPSModelArtifactItem",
          "description": "The Tensorflow checkpoint files. e.g. Used for resumable training."
        },
        "exportArtifact": {
          "description": "The model binary files in different formats for model export.",
          "items": {
            "$ref": "XPSModelArtifactItem"
          },
          "type": "array"
        },
        "tfJsBinaryGcsPrefix": {
          "description": "GCS uri prefix of Tensorflow JavaScript binary files 'groupX-shardXofX.bin' Deprecated.",
          "type": "string"
        },
        "tfLiteMetadataGcsUri": {
          "description": "GCS uri of Tensorflow Lite metadata 'tflite_metadata.json'.",
          "type": "string"
        },
        "servingArtifact": {
          "description": "The default model binary file used for serving (e.g. online predict, batch predict) via public Cloud AI Platform API.",
          "$ref": "XPSModelArtifactItem"
        },
        "labelGcsUri": {
          "description": "GCS uri of decoded labels file for model export 'dict.txt'.",
          "type": "string"
        }
      },
      "type": "object",
      "description": "Stores the locations and related metadata of the model artifacts. Populated for uCAIP requests only."
    },
    "DependencyEdge": {
      "type": "object",
      "properties": {
        "headTokenIndex": {
          "description": "Represents the head of this token in the dependency tree. This is the index of the token which has an arc going to this token. The index is the position of the token in the array of tokens returned by the API method. If this token is a root token, then the `head_token_index` is its own index.",
          "format": "int32",
          "type": "integer"
        },
        "label": {
          "enum": [
            "UNKNOWN",
            "ABBREV",
            "ACOMP",
            "ADVCL",
            "ADVMOD",
            "AMOD",
            "APPOS",
            "ATTR",
            "AUX",
            "AUXPASS",
            "CC",
            "CCOMP",
            "CONJ",
            "CSUBJ",
            "CSUBJPASS",
            "DEP",
            "DET",
            "DISCOURSE",
            "DOBJ",
            "EXPL",
            "GOESWITH",
            "IOBJ",
            "MARK",
            "MWE",
            "MWV",
            "NEG",
            "NN",
            "NPADVMOD",
            "NSUBJ",
            "NSUBJPASS",
            "NUM",
            "NUMBER",
            "P",
            "PARATAXIS",
            "PARTMOD",
            "PCOMP",
            "POBJ",
            "POSS",
            "POSTNEG",
            "PRECOMP",
            "PRECONJ",
            "PREDET",
            "PREF",
            "PREP",
            "PRONL",
            "PRT",
            "PS",
            "QUANTMOD",
            "RCMOD",
            "RCMODREL",
            "RDROP",
            "REF",
            "REMNANT",
            "REPARANDUM",
            "ROOT",
            "SNUM",
            "SUFF",
            "TMOD",
            "TOPIC",
            "VMOD",
            "VOCATIVE",
            "XCOMP",
            "SUFFIX",
            "TITLE",
            "ADVPHMOD",
            "AUXCAUS",
            "AUXVV",
            "DTMOD",
            "FOREIGN",
            "KW",
            "LIST",
            "NOMC",
            "NOMCSUBJ",
            "NOMCSUBJPASS",
            "NUMC",
            "COP",
            "DISLOCATED",
            "ASP",
            "GMOD",
            "GOBJ",
            "INFMOD",
            "MES",
            "NCOMP"
          ],
          "type": "string",
          "enumDescriptions": [
            "Unknown",
            "Abbreviation modifier",
            "Adjectival complement",
            "Adverbial clause modifier",
            "Adverbial modifier",
            "Adjectival modifier of an NP",
            "Appositional modifier of an NP",
            "Attribute dependent of a copular verb",
            "Auxiliary (non-main) verb",
            "Passive auxiliary",
            "Coordinating conjunction",
            "Clausal complement of a verb or adjective",
            "Conjunct",
            "Clausal subject",
            "Clausal passive subject",
            "Dependency (unable to determine)",
            "Determiner",
            "Discourse",
            "Direct object",
            "Expletive",
            "Goes with (part of a word in a text not well edited)",
            "Indirect object",
            "Marker (word introducing a subordinate clause)",
            "Multi-word expression",
            "Multi-word verbal expression",
            "Negation modifier",
            "Noun compound modifier",
            "Noun phrase used as an adverbial modifier",
            "Nominal subject",
            "Passive nominal subject",
            "Numeric modifier of a noun",
            "Element of compound number",
            "Punctuation mark",
            "Parataxis relation",
            "Participial modifier",
            "The complement of a preposition is a clause",
            "Object of a preposition",
            "Possession modifier",
            "Postverbal negative particle",
            "Predicate complement",
            "Preconjunt",
            "Predeterminer",
            "Prefix",
            "Prepositional modifier",
            "The relationship between a verb and verbal morpheme",
            "Particle",
            "Associative or possessive marker",
            "Quantifier phrase modifier",
            "Relative clause modifier",
            "Complementizer in relative clause",
            "Ellipsis without a preceding predicate",
            "Referent",
            "Remnant",
            "Reparandum",
            "Root",
            "Suffix specifying a unit of number",
            "Suffix",
            "Temporal modifier",
            "Topic marker",
            "Clause headed by an infinite form of the verb that modifies a noun",
            "Vocative",
            "Open clausal complement",
            "Name suffix",
            "Name title",
            "Adverbial phrase modifier",
            "Causative auxiliary",
            "Helper auxiliary",
            "Rentaishi (Prenominal modifier)",
            "Foreign words",
            "Keyword",
            "List for chains of comparable items",
            "Nominalized clause",
            "Nominalized clausal subject",
            "Nominalized clausal passive",
            "Compound of numeric modifier",
            "Copula",
            "Dislocated relation (for fronted/topicalized elements)",
            "Aspect marker",
            "Genitive modifier",
            "Genitive object",
            "Infinitival modifier",
            "Measure",
            "Nominal complement of a noun"
          ],
          "description": "The parse label for the token."
        }
      },
      "description": "Represents dependency parse tree information for a token.",
      "id": "DependencyEdge"
    },
    "XPSTranslationPreprocessResponse": {
      "id": "XPSTranslationPreprocessResponse",
      "type": "object",
      "properties": {
        "validExampleCount": {
          "format": "int64",
          "description": "Total valid example count.",
          "type": "string"
        },
        "parsedExampleCount": {
          "format": "int64",
          "type": "string",
          "description": "Total example count parsed."
        }
      },
      "description": "Translation preprocess response."
    },
    "XPSConfusionMatrixRow": {
      "description": "A row in the confusion matrix.",
      "type": "object",
      "id": "XPSConfusionMatrixRow",
      "properties": {
        "exampleCount": {
          "type": "array",
          "description": "Value of the specific cell in the confusion matrix. The number of values each row has (i.e. the length of the row) is equal to the length of the annotation_spec_id_token field.",
          "items": {
            "type": "integer",
            "format": "int32"
          }
        },
        "count": {
          "type": "array",
          "description": "Same as above except intended to represent other counts (for e.g. for segmentation this is pixel count). NOTE(params): Only example_count or count is set (oneoff does not support repeated fields unless they are embedded inside another message).",
          "items": {
            "type": "string",
            "format": "int64"
          }
        }
      }
    },
    "XPSReportingMetrics": {
      "type": "object",
      "properties": {
        "effectiveTrainingDuration": {
          "description": "The effective time training used. If set, this is used for quota management and billing. Deprecated. AutoML BE doesn't use this. Don't set.",
          "type": "string",
          "deprecated": true,
          "format": "google-duration"
        },
        "metricEntries": {
          "type": "array",
          "items": {
            "$ref": "XPSMetricEntry"
          },
          "description": "One entry per metric name. The values must be aggregated per metric name."
        }
      },
      "id": "XPSReportingMetrics"
    },
    "XPSTablesPreprocessResponse": {
      "id": "XPSTablesPreprocessResponse",
      "properties": {
        "tablesDatasetMetadata": {
          "description": "The table/column id, column_name and the DataTypes of the columns will be populated.",
          "$ref": "XPSTablesDatasetMetadata"
        }
      },
      "type": "object"
    },
    "AnnotateTextRequest": {
      "type": "object",
      "id": "AnnotateTextRequest",
      "description": "The request message for the text annotation API, which can perform multiple analysis types (sentiment, entities, and syntax) in one call.",
      "properties": {
        "encodingType": {
          "enum": [
            "NONE",
            "UTF8",
            "UTF16",
            "UTF32"
          ],
          "type": "string",
          "enumDescriptions": [
            "If `EncodingType` is not specified, encoding-dependent information (such as `begin_offset`) will be set at `-1`.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-8 encoding of the input. C++ and Go are examples of languages that use this encoding natively.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-16 encoding of the input. Java and JavaScript are examples of languages that use this encoding natively.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-32 encoding of the input. Python is an example of a language that uses this encoding natively."
          ],
          "description": "The encoding type used by the API to calculate offsets."
        },
        "document": {
          "$ref": "Document",
          "description": "Required. Input document."
        },
        "features": {
          "description": "Required. The enabled features.",
          "$ref": "AnnotateTextRequestFeatures"
        }
      }
    },
    "XPSTextToSpeechTrainResponse": {
      "id": "XPSTextToSpeechTrainResponse",
      "properties": {},
      "description": "TextToSpeech train response",
      "type": "object"
    },
    "XPSEdgeTpuTfLiteFormat": {
      "id": "XPSEdgeTpuTfLiteFormat",
      "properties": {},
      "type": "object",
      "description": "A model format used for [Edge TPU](https://cloud.google.com/edge-tpu/) devices."
    },
    "XPSTfSavedModelFormat": {
      "id": "XPSTfSavedModelFormat",
      "type": "object",
      "description": "A tensorflow model format in SavedModel format.",
      "properties": {}
    },
    "XPSModelArtifactItem": {
      "id": "XPSModelArtifactItem",
      "properties": {
        "gcsUri": {
          "description": "The Google Cloud Storage (GCS) uri that stores the model binary files.",
          "type": "string"
        },
        "artifactFormat": {
          "type": "string",
          "description": "The model artifact format.",
          "enumDescriptions": [
            "Should not be used.",
            "The Tensorflow checkpoints. See https://www.tensorflow.org/guide/checkpoint.",
            "The Tensorflow SavedModel binary.",
            "Model artifact in generic TensorFlow Lite (.tflite) format. See https://www.tensorflow.org/lite.",
            "Used for [Edge TPU](https://cloud.google.com/edge-tpu/) devices.",
            "A [TensorFlow.js](https://www.tensorflow.org/js) model that can be used in the browser and in Node.js using JavaScript.",
            "Used for iOS mobile devices in (.mlmodel) format. See https://developer.apple.com/documentation/coreml"
          ],
          "enum": [
            "ARTIFACT_FORMAT_UNSPECIFIED",
            "TF_CHECKPOINT",
            "TF_SAVED_MODEL",
            "TF_LITE",
            "EDGE_TPU_TF_LITE",
            "TF_JS",
            "CORE_ML"
          ]
        }
      },
      "description": "A single model artifact item.",
      "type": "object"
    },
    "RamMetric": {
      "id": "RamMetric",
      "properties": {
        "memories": {
          "format": "double",
          "type": "number",
          "description": "Required. VM memory in gb."
        },
        "gibSec": {
          "type": "string",
          "description": "Required. VM memory in Gigabyte second, e.g. 3600. Using int64 type to match billing metrics definition.",
          "format": "int64"
        },
        "trackingLabels": {
          "description": "Billing tracking labels. They do not contain any user data but only the labels set by Vertex Core Infra itself. Tracking labels' keys are defined with special format: goog-[\\p{Ll}\\p{N}]+ E.g. \"key\": \"goog-k8s-cluster-name\",\"value\": \"us-east1-b4rk\"",
          "type": "object",
          "additionalProperties": {
            "type": "string"
          }
        },
        "machineSpec": {
          "enumDescriptions": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
          ],
          "enum": [
            "UNKNOWN_MACHINE_SPEC",
            "N1_STANDARD_2",
            "N1_STANDARD_4",
            "N1_STANDARD_8",
            "N1_STANDARD_16",
            "N1_STANDARD_32",
            "N1_STANDARD_64",
            "N1_STANDARD_96",
            "N1_HIGHMEM_2",
            "N1_HIGHMEM_4",
            "N1_HIGHMEM_8",
            "N1_HIGHMEM_16",
            "N1_HIGHMEM_32",
            "N1_HIGHMEM_64",
            "N1_HIGHMEM_96",
            "N1_HIGHCPU_2",
            "N1_HIGHCPU_4",
            "N1_HIGHCPU_8",
            "N1_HIGHCPU_16",
            "N1_HIGHCPU_32",
            "N1_HIGHCPU_64",
            "N1_HIGHCPU_96",
            "A2_HIGHGPU_1G",
            "A2_HIGHGPU_2G",
            "A2_HIGHGPU_4G",
            "A2_HIGHGPU_8G",
            "A2_MEGAGPU_16G",
            "A2_ULTRAGPU_1G",
            "A2_ULTRAGPU_2G",
            "A2_ULTRAGPU_4G",
            "A2_ULTRAGPU_8G",
            "A3_HIGHGPU_8G",
            "E2_STANDARD_2",
            "E2_STANDARD_4",
            "E2_STANDARD_8",
            "E2_STANDARD_16",
            "E2_STANDARD_32",
            "E2_HIGHMEM_2",
            "E2_HIGHMEM_4",
            "E2_HIGHMEM_8",
            "E2_HIGHMEM_16",
            "E2_HIGHCPU_2",
            "E2_HIGHCPU_4",
            "E2_HIGHCPU_8",
            "E2_HIGHCPU_16",
            "E2_HIGHCPU_32",
            "N2_STANDARD_2",
            "N2_STANDARD_4",
            "N2_STANDARD_8",
            "N2_STANDARD_16",
            "N2_STANDARD_32",
            "N2_STANDARD_48",
            "N2_STANDARD_64",
            "N2_STANDARD_80",
            "N2_STANDARD_96",
            "N2_STANDARD_128",
            "N2_HIGHMEM_2",
            "N2_HIGHMEM_4",
            "N2_HIGHMEM_8",
            "N2_HIGHMEM_16",
            "N2_HIGHMEM_32",
            "N2_HIGHMEM_48",
            "N2_HIGHMEM_64",
            "N2_HIGHMEM_80",
            "N2_HIGHMEM_96",
            "N2_HIGHMEM_128",
            "N2_HIGHCPU_2",
            "N2_HIGHCPU_4",
            "N2_HIGHCPU_8",
            "N2_HIGHCPU_16",
            "N2_HIGHCPU_32",
            "N2_HIGHCPU_48",
            "N2_HIGHCPU_64",
            "N2_HIGHCPU_80",
            "N2_HIGHCPU_96",
            "N2D_STANDARD_2",
            "N2D_STANDARD_4",
            "N2D_STANDARD_8",
            "N2D_STANDARD_16",
            "N2D_STANDARD_32",
            "N2D_STANDARD_48",
            "N2D_STANDARD_64",
            "N2D_STANDARD_80",
            "N2D_STANDARD_96",
            "N2D_STANDARD_128",
            "N2D_STANDARD_224",
            "N2D_HIGHMEM_2",
            "N2D_HIGHMEM_4",
            "N2D_HIGHMEM_8",
            "N2D_HIGHMEM_16",
            "N2D_HIGHMEM_32",
            "N2D_HIGHMEM_48",
            "N2D_HIGHMEM_64",
            "N2D_HIGHMEM_80",
            "N2D_HIGHMEM_96",
            "N2D_HIGHCPU_2",
            "N2D_HIGHCPU_4",
            "N2D_HIGHCPU_8",
            "N2D_HIGHCPU_16",
            "N2D_HIGHCPU_32",
            "N2D_HIGHCPU_48",
            "N2D_HIGHCPU_64",
            "N2D_HIGHCPU_80",
            "N2D_HIGHCPU_96",
            "N2D_HIGHCPU_128",
            "N2D_HIGHCPU_224",
            "C2_STANDARD_4",
            "C2_STANDARD_8",
            "C2_STANDARD_16",
            "C2_STANDARD_30",
            "C2_STANDARD_60",
            "C2D_STANDARD_2",
            "C2D_STANDARD_4",
            "C2D_STANDARD_8",
            "C2D_STANDARD_16",
            "C2D_STANDARD_32",
            "C2D_STANDARD_56",
            "C2D_STANDARD_112",
            "C2D_HIGHCPU_2",
            "C2D_HIGHCPU_4",
            "C2D_HIGHCPU_8",
            "C2D_HIGHCPU_16",
            "C2D_HIGHCPU_32",
            "C2D_HIGHCPU_56",
            "C2D_HIGHCPU_112",
            "C2D_HIGHMEM_2",
            "C2D_HIGHMEM_4",
            "C2D_HIGHMEM_8",
            "C2D_HIGHMEM_16",
            "C2D_HIGHMEM_32",
            "C2D_HIGHMEM_56",
            "C2D_HIGHMEM_112",
            "G2_STANDARD_4",
            "G2_STANDARD_8",
            "G2_STANDARD_12",
            "G2_STANDARD_16",
            "G2_STANDARD_24",
            "G2_STANDARD_32",
            "G2_STANDARD_48",
            "G2_STANDARD_96",
            "C3_STANDARD_4",
            "C3_STANDARD_8",
            "C3_STANDARD_22",
            "C3_STANDARD_44",
            "C3_STANDARD_88",
            "C3_STANDARD_176",
            "C3_HIGHCPU_4",
            "C3_HIGHCPU_8",
            "C3_HIGHCPU_22",
            "C3_HIGHCPU_44",
            "C3_HIGHCPU_88",
            "C3_HIGHCPU_176",
            "C3_HIGHMEM_4",
            "C3_HIGHMEM_8",
            "C3_HIGHMEM_22",
            "C3_HIGHMEM_44",
            "C3_HIGHMEM_88",
            "C3_HIGHMEM_176"
          ],
          "type": "string",
          "description": "Required. Machine spec, e.g. N1_STANDARD_4."
        },
        "ramType": {
          "description": "Required. Type of ram.",
          "enumDescriptions": [
            "",
            "",
            "",
            "COMPUTE_OPTIMIZED",
            "",
            "",
            "",
            "",
            "",
            "MEMORY_OPTIMIZED_UPGRADE_PREMIUM",
            "MEMORY_OPTIMIZED",
            "",
            "",
            "",
            ""
          ],
          "type": "string",
          "enum": [
            "UNKNOWN_RAM_TYPE",
            "A2",
            "A3",
            "C2",
            "C2D",
            "CUSTOM",
            "E2",
            "G2",
            "C3",
            "M2",
            "M1",
            "N1",
            "N2_CUSTOM",
            "N2",
            "N2D"
          ]
        }
      },
      "type": "object"
    },
    "XPSXraiAttribution": {
      "description": "An explanation method that redistributes Integrated Gradients attributions to segmented regions, taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1906.02825 Only supports image Models (modality is IMAGE).",
      "deprecated": true,
      "id": "XPSXraiAttribution",
      "type": "object",
      "properties": {
        "stepCount": {
          "description": "The number of steps for approximating the path integral. A good value to start is 50 and gradually increase until the sum to diff property is met within the desired error range. Valid range of its value is [1, 100], inclusively.",
          "format": "int32",
          "type": "integer"
        }
      }
    },
    "XPSVideoModelArtifactSpec": {
      "type": "object",
      "id": "XPSVideoModelArtifactSpec",
      "properties": {
        "exportArtifact": {
          "items": {
            "$ref": "XPSModelArtifactItem"
          },
          "type": "array",
          "description": "The model binary files in different formats for model export."
        },
        "servingArtifact": {
          "$ref": "XPSModelArtifactItem",
          "description": "The default model binary file used for serving (e.g. batch predict) via public Cloud AI Platform API."
        }
      }
    },
    "ClassifyTextResponse": {
      "properties": {
        "categories": {
          "description": "Categories representing the input document.",
          "items": {
            "$ref": "ClassificationCategory"
          },
          "type": "array"
        }
      },
      "type": "object",
      "description": "The document classification response message.",
      "id": "ClassifyTextResponse"
    },
    "XPSResponseExplanationParameters": {
      "type": "object",
      "deprecated": true,
      "id": "XPSResponseExplanationParameters",
      "properties": {
        "integratedGradientsAttribution": {
          "$ref": "XPSIntegratedGradientsAttribution",
          "description": "An attribution method that computes Aumann-Shapley values taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1703.01365"
        },
        "xraiAttribution": {
          "description": "An attribution method that redistributes Integrated Gradients attribution to segmented regions, taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1906.02825 XRAI currently performs better on natural images, like a picture of a house or an animal. If the images are taken in artificial environments, like a lab or manufacturing line, or from diagnostic equipment, like x-rays or quality-control cameras, use Integrated Gradients instead.",
          "$ref": "XPSXraiAttribution"
        }
      }
    },
    "AnalyzeEntitiesResponse": {
      "description": "The entity analysis response message.",
      "id": "AnalyzeEntitiesResponse",
      "properties": {
        "language": {
          "description": "The language of the text, which will be the same as the language specified in the request or, if not specified, the automatically-detected language. See Document.language field for more details.",
          "type": "string"
        },
        "entities": {
          "type": "array",
          "items": {
            "$ref": "Entity"
          },
          "description": "The recognized entities in the input document."
        }
      },
      "type": "object"
    },
    "ModerateTextRequest": {
      "type": "object",
      "description": "The document moderation request message.",
      "id": "ModerateTextRequest",
      "properties": {
        "document": {
          "$ref": "Document",
          "description": "Required. Input document."
        }
      }
    },
    "XPSImageSegmentationEvaluationMetrics": {
      "properties": {
        "confidenceMetricsEntries": {
          "description": "Metrics that have confidence thresholds. Precision-recall curve can be derived from it.",
          "items": {
            "$ref": "XPSImageSegmentationEvaluationMetricsConfidenceMetricsEntry"
          },
          "type": "array"
        }
      },
      "type": "object",
      "id": "XPSImageSegmentationEvaluationMetrics",
      "description": "Model evaluation metrics for image segmentation problems. Next tag: 4."
    },
    "XPSTimestampStatsGranularStats": {
      "properties": {
        "buckets": {
          "type": "object",
          "additionalProperties": {
            "format": "int64",
            "type": "string"
          },
          "description": "A map from granularity key to example count for that key. E.g. for hour_of_day `13` means 1pm, or for month_of_year `5` means May)."
        }
      },
      "id": "XPSTimestampStatsGranularStats",
      "description": "Stats split by a defined in context granularity.",
      "type": "object"
    },
    "XPSVideoObjectTrackingTrainResponse": {
      "id": "XPSVideoObjectTrackingTrainResponse",
      "type": "object",
      "properties": {
        "exportModelSpec": {
          "$ref": "XPSVideoExportModelSpec",
          "description": "Populated for AutoML request only."
        },
        "trainCostNodeSeconds": {
          "format": "int64",
          "type": "string",
          "description": "The actual train cost of creating this model, expressed in node seconds, i.e. 3,600 value in this field means 1 node hour."
        },
        "modelArtifactSpec": {
          "$ref": "XPSVideoModelArtifactSpec",
          "description": "## The fields below are only populated under uCAIP request scope."
        }
      }
    },
    "XPSTablesModelStructure": {
      "id": "XPSTablesModelStructure",
      "properties": {
        "modelParameters": {
          "type": "array",
          "description": "A list of models.",
          "items": {
            "$ref": "XPSTablesModelStructureModelParameters"
          }
        }
      },
      "type": "object",
      "description": "A description of Tables model structure."
    },
    "ClassificationModelOptionsV2Model": {
      "id": "ClassificationModelOptionsV2Model",
      "type": "object",
      "description": "Options for the V2 model.",
      "properties": {
        "contentCategoriesVersion": {
          "type": "string",
          "enumDescriptions": [
            "If `ContentCategoriesVersion` is not specified, this option will default to `V1`.",
            "Legacy content categories of our initial launch in 2017.",
            "Updated content categories in 2022."
          ],
          "description": "The content categories used for classification.",
          "enum": [
            "CONTENT_CATEGORIES_VERSION_UNSPECIFIED",
            "V1",
            "V2"
          ]
        }
      }
    },
    "AnnotateTextResponse": {
      "id": "AnnotateTextResponse",
      "description": "The text annotations response message.",
      "type": "object",
      "properties": {
        "documentSentiment": {
          "$ref": "Sentiment",
          "description": "The overall sentiment for the document. Populated if the user enables AnnotateTextRequest.Features.extract_document_sentiment."
        },
        "language": {
          "type": "string",
          "description": "The language of the text, which will be the same as the language specified in the request or, if not specified, the automatically-detected language. See Document.language field for more details."
        },
        "sentences": {
          "items": {
            "$ref": "Sentence"
          },
          "description": "Sentences in the input document. Populated if the user enables AnnotateTextRequest.Features.extract_syntax.",
          "type": "array"
        },
        "moderationCategories": {
          "items": {
            "$ref": "ClassificationCategory"
          },
          "type": "array",
          "description": "Harmful and sensitive categories identified in the input document."
        },
        "categories": {
          "description": "Categories identified in the input document.",
          "items": {
            "$ref": "ClassificationCategory"
          },
          "type": "array"
        },
        "tokens": {
          "type": "array",
          "description": "Tokens, along with their syntactic information, in the input document. Populated if the user enables AnnotateTextRequest.Features.extract_syntax.",
          "items": {
            "$ref": "Token"
          }
        },
        "entities": {
          "items": {
            "$ref": "Entity"
          },
          "type": "array",
          "description": "Entities, along with their semantic information, in the input document. Populated if the user enables AnnotateTextRequest.Features.extract_entities."
        }
      }
    },
    "XPSColumnSpecForecastingMetadata": {
      "type": "object",
      "properties": {
        "columnType": {
          "description": "The type of the column for FORECASTING model training purposes.",
          "enumDescriptions": [
            "An un-set value of this enum.",
            "Key columns are used to identify timeseries.",
            "This column contains information describing static properties of the entities identified by the key column(s) (e.g. city's ZIP code).",
            "This column contains information for the given entity, at any time poinrt, they are only available in the time series before.",
            "This column contains information for the given entity is known both for the past and the sufficiently far future."
          ],
          "enum": [
            "COLUMN_TYPE_UNSPECIFIED",
            "KEY",
            "KEY_METADATA",
            "TIME_SERIES_AVAILABLE_PAST_ONLY",
            "TIME_SERIES_AVAILABLE_PAST_AND_FUTURE"
          ],
          "type": "string"
        }
      },
      "description": "=========================================================================== # The fields below are used exclusively for Forecasting.",
      "id": "XPSColumnSpecForecastingMetadata"
    },
    "AnalyzeEntitySentimentRequest": {
      "properties": {
        "encodingType": {
          "description": "The encoding type used by the API to calculate offsets.",
          "enumDescriptions": [
            "If `EncodingType` is not specified, encoding-dependent information (such as `begin_offset`) will be set at `-1`.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-8 encoding of the input. C++ and Go are examples of languages that use this encoding natively.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-16 encoding of the input. Java and JavaScript are examples of languages that use this encoding natively.",
            "Encoding-dependent information (such as `begin_offset`) is calculated based on the UTF-32 encoding of the input. Python is an example of a language that uses this encoding natively."
          ],
          "enum": [
            "NONE",
            "UTF8",
            "UTF16",
            "UTF32"
          ],
          "type": "string"
        },
        "document": {
          "description": "Required. Input document.",
          "$ref": "Document"
        }
      },
      "type": "object",
      "id": "AnalyzeEntitySentimentRequest",
      "description": "The entity-level sentiment analysis request message."
    }
  },
  "version_module": true,
  "name": "language",
  "discoveryVersion": "v1",
  "parameters": {
    "quotaUser": {
      "location": "query",
      "description": "Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.",
      "type": "string"
    },
    "fields": {
      "description": "Selector specifying which fields to include in a partial response.",
      "location": "query",
      "type": "string"
    },
    "prettyPrint": {
      "default": "true",
      "location": "query",
      "description": "Returns response with indentations and line breaks.",
      "type": "boolean"
    },
    "callback": {
      "location": "query",
      "type": "string",
      "description": "JSONP"
    },
    "oauth_token": {
      "description": "OAuth 2.0 token for the current user.",
      "type": "string",
      "location": "query"
    },
    "access_token": {
      "description": "OAuth access token.",
      "location": "query",
      "type": "string"
    },
    "key": {
      "location": "query",
      "description": "API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.",
      "type": "string"
    },
    "uploadType": {
      "type": "string",
      "description": "Legacy upload protocol for media (e.g. \"media\", \"multipart\").",
      "location": "query"
    },
    "$.xgafv": {
      "description": "V1 error format.",
      "location": "query",
      "enum": [
        "1",
        "2"
      ],
      "type": "string",
      "enumDescriptions": [
        "v1 error format",
        "v2 error format"
      ]
    },
    "upload_protocol": {
      "location": "query",
      "type": "string",
      "description": "Upload protocol for media (e.g. \"raw\", \"multipart\")."
    },
    "alt": {
      "type": "string",
      "enum": [
        "json",
        "media",
        "proto"
      ],
      "location": "query",
      "enumDescriptions": [
        "Responses with Content-Type of application/json",
        "Media download with context-dependent Content-Type",
        "Responses with Content-Type of application/x-protobuf"
      ],
      "default": "json",
      "description": "Data format for response."
    }
  },
  "revision": "20240220",
  "id": "language:v1beta2",
  "protocol": "rest",
  "mtlsRootUrl": "https://language.mtls.googleapis.com/",
  "baseUrl": "https://language.googleapis.com/",
  "canonicalName": "Cloud Natural Language",
  "description": "Provides natural language understanding technologies, such as sentiment analysis, entity recognition, entity sentiment analysis, and other text annotations, to developers.",
  "auth": {
    "oauth2": {
      "scopes": {
        "https://www.googleapis.com/auth/cloud-language": {
          "description": "Apply machine learning models to reveal the structure and meaning of text"
        },
        "https://www.googleapis.com/auth/cloud-platform": {
          "description": "See, edit, configure, and delete your Google Cloud data and see the email address for your Google Account."
        }
      }
    }
  },
  "title": "Cloud Natural Language API",
  "basePath": "",
  "batchPath": "batch",
  "version": "v1beta2"
}
